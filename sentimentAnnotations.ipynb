{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7a6ed2d",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import optuna as optuna\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f300bf58",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1872080baa7d30ec8fb87be9a65358cd3a7fb649&gt;894be...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>However, how frataxin interacts with the Fe-S ...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b&gt;b6642...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In the study by Hickey et al. (2012), spikes w...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>9cdf605beb1aa1078f235c4332b3024daa8b31dc&gt;4e6a1...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The drug also reduces catecholamine secretion,...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d9f3207db0c79a3b154f3875c9760cc6b056904b&gt;2cc6f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>By clustering with lowly aggressive close kin ...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>88b86556857f4374842d2af2e359576806239175&gt;a5bb0...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Ophthalmic symptoms are rare manifestations of...</td>\n",
       "      <td>background</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       citingPaperID    source  \\\n",
       "0  1872080baa7d30ec8fb87be9a65358cd3a7fb649>894be...  explicit   \n",
       "1  ce1d09a4a3a8d7fd3405b9328f65f00c952cf64b>b6642...  explicit   \n",
       "2  9cdf605beb1aa1078f235c4332b3024daa8b31dc>4e6a1...  explicit   \n",
       "3  d9f3207db0c79a3b154f3875c9760cc6b056904b>2cc6f...  explicit   \n",
       "4  88b86556857f4374842d2af2e359576806239175>a5bb0...  explicit   \n",
       "\n",
       "                                              string       label  \n",
       "0  However, how frataxin interacts with the Fe-S ...  background  \n",
       "1  In the study by Hickey et al. (2012), spikes w...  background  \n",
       "2  The drug also reduces catecholamine secretion,...  background  \n",
       "3  By clustering with lowly aggressive close kin ...  background  \n",
       "4  Ophthalmic symptoms are rare manifestations of...  background  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Read in training dataset (TSV)\n",
    "# Filepath will be different; set to repo's filepath when we add the corpus files\n",
    "train_tsv = pd.read_csv('scicite/tsv/train.tsv', sep='\\t', \n",
    "                       names=[\"citingPaperID\", \"source\", \"string\", \"label\"]\n",
    "                       )\n",
    "train_tsv.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1da1964",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# First set of sentiment annotations (Week of Jun. 27, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "63da7017",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Getting samples of the data for sentiment annotating\n",
    "train_sentiment = train_tsv.copy()\n",
    "\n",
    "# Chris' subset\n",
    "c_sample = list(train_sentiment['string'].sample(n=50, random_state=1).index)\n",
    "# Drop\n",
    "train_sentiment = train_sentiment.drop(c_sample)\n",
    "\n",
    "# Daniel's subset\n",
    "d_sample = list(train_sentiment['string'].sample(n=50, random_state=1).index)\n",
    "# Drop\n",
    "train_sentiment = train_sentiment.drop(d_sample)\n",
    "\n",
    "# Mutual subset (for IAA score)\n",
    "m_sample = list(train_sentiment['string'].sample(n=25, random_state=1).index)\n",
    "train_sentiment = train_sentiment.drop(m_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bbe5681",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2305</th>\n",
       "      <td>f6e7d88342d23cdc6dc959b16106b6bb05fa807a&gt;0fee7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Utilizing gain-offunction assays, we demonstra...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3210</th>\n",
       "      <td>51439b498621806aa3a915fdbd4aa983473da397&gt;6eb72...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>maticity, refractoriness and conduction of the...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4484</th>\n",
       "      <td>64cf98067d1cadda174e5ffa24fc45a6b3e6426f&gt;2d04a...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>For reovirus 1/L-induced ARDS, at day 9 postin...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5821</th>\n",
       "      <td>9b6d433ab5104713ee41fbeb24b4d0f2b82fe587&gt;95158...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Besides that, no ameliorate impacts have been ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3514</th>\n",
       "      <td>15ac5bcd9ca900dc948e969558670084e3d560e0&gt;5297c...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>However, limited information is available abou...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID    source  \\\n",
       "2305  f6e7d88342d23cdc6dc959b16106b6bb05fa807a>0fee7...  explicit   \n",
       "3210  51439b498621806aa3a915fdbd4aa983473da397>6eb72...  explicit   \n",
       "4484  64cf98067d1cadda174e5ffa24fc45a6b3e6426f>2d04a...  explicit   \n",
       "5821  9b6d433ab5104713ee41fbeb24b4d0f2b82fe587>95158...  explicit   \n",
       "3514  15ac5bcd9ca900dc948e969558670084e3d560e0>5297c...  explicit   \n",
       "\n",
       "                                                 string       label sentiment  \n",
       "2305  Utilizing gain-offunction assays, we demonstra...      method            \n",
       "3210  maticity, refractoriness and conduction of the...  background            \n",
       "4484  For reovirus 1/L-induced ARDS, at day 9 postin...  background            \n",
       "5821  Besides that, no ameliorate impacts have been ...  background            \n",
       "3514  However, limited information is available abou...  background            "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chris' subset in DataFrame form (for adding a sentiment column/feature)\n",
    "c_df = train_tsv.copy()\n",
    "c_df = c_df.iloc[c_sample]\n",
    "c_df['sentiment'] = ['']*50\n",
    "c_df.to_csv('sentimentAnnotations_CSV/blankAnnotations_C_1.csv')\n",
    "c_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1cfd3e46",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2305</td>\n",
       "      <td>f6e7d88342d23cdc6dc959b16106b6bb05fa807a&gt;0fee7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Utilizing gain-offunction assays, we demonstra...</td>\n",
       "      <td>method</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3210</td>\n",
       "      <td>51439b498621806aa3a915fdbd4aa983473da397&gt;6eb72...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>maticity, refractoriness and conduction of the...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4484</td>\n",
       "      <td>64cf98067d1cadda174e5ffa24fc45a6b3e6426f&gt;2d04a...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>For reovirus 1/L-induced ARDS, at day 9 postin...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5821</td>\n",
       "      <td>9b6d433ab5104713ee41fbeb24b4d0f2b82fe587&gt;95158...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Besides that, no ameliorate impacts have been ...</td>\n",
       "      <td>background</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3514</td>\n",
       "      <td>15ac5bcd9ca900dc948e969558670084e3d560e0&gt;5297c...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>However, limited information is available abou...</td>\n",
       "      <td>background</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID    source  \\\n",
       "0        2305  f6e7d88342d23cdc6dc959b16106b6bb05fa807a>0fee7...  explicit   \n",
       "1        3210  51439b498621806aa3a915fdbd4aa983473da397>6eb72...  explicit   \n",
       "2        4484  64cf98067d1cadda174e5ffa24fc45a6b3e6426f>2d04a...  explicit   \n",
       "3        5821  9b6d433ab5104713ee41fbeb24b4d0f2b82fe587>95158...  explicit   \n",
       "4        3514  15ac5bcd9ca900dc948e969558670084e3d560e0>5297c...  explicit   \n",
       "\n",
       "                                              string       label sentiment  \n",
       "0  Utilizing gain-offunction assays, we demonstra...      method  positive  \n",
       "1  maticity, refractoriness and conduction of the...  background   neutral  \n",
       "2  For reovirus 1/L-induced ARDS, at day 9 postin...  background   neutral  \n",
       "3  Besides that, no ameliorate impacts have been ...  background  negative  \n",
       "4  However, limited information is available abou...  background  negative  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in Chris' subset (after annotating manually)\n",
    "c_df_annotated = pd.read_csv('sentimentAnnotations_CSV/annotated_C_1.csv')\n",
    "c_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e3f2b65a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3923</th>\n",
       "      <td>6828a9cc19290ccd8cc99ccc678a042b00d92125&gt;26cad...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The relationship between DNA content and absor...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4306</th>\n",
       "      <td>5cecd1d9932d3f269b1caa45d07e84e3376c3710&gt;9a0f5...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>With respect to the contradicting results of o...</td>\n",
       "      <td>result</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4960</th>\n",
       "      <td>1890ccd4d2a4d542ba24081b593ab8060e38f101&gt;12732...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>We elected to take this course because formal ...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6921</th>\n",
       "      <td>e2e9ce75aab15707ff3b85378b61f161c0b7886a&gt;39688...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Nuclear localization of PTOV1 is required for ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3526</th>\n",
       "      <td>afe94f58975df56a9c0450c8153c2dd43d0b597b&gt;ec694...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The remaining medial portion of the bone was p...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID    source  \\\n",
       "3923  6828a9cc19290ccd8cc99ccc678a042b00d92125>26cad...  explicit   \n",
       "4306  5cecd1d9932d3f269b1caa45d07e84e3376c3710>9a0f5...  explicit   \n",
       "4960  1890ccd4d2a4d542ba24081b593ab8060e38f101>12732...  explicit   \n",
       "6921  e2e9ce75aab15707ff3b85378b61f161c0b7886a>39688...  explicit   \n",
       "3526  afe94f58975df56a9c0450c8153c2dd43d0b597b>ec694...  explicit   \n",
       "\n",
       "                                                 string       label sentiment  \n",
       "3923  The relationship between DNA content and absor...  background            \n",
       "4306  With respect to the contradicting results of o...      result            \n",
       "4960  We elected to take this course because formal ...      method            \n",
       "6921  Nuclear localization of PTOV1 is required for ...  background            \n",
       "3526  The remaining medial portion of the bone was p...      method            "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daniel's subset in DataFrame form (for adding a sentiment column/feature)\n",
    "d_df = train_tsv.copy()\n",
    "d_df = d_df.iloc[d_sample]\n",
    "d_df['sentiment'] = ['']*50\n",
    "d_df.to_csv('sentimentAnnotations_CSV/blankAnnotations_D_1.csv')\n",
    "d_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d012dd8c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3923</td>\n",
       "      <td>6828a9cc19290ccd8cc99ccc678a042b00d92125&gt;26cad...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The relationship between DNA content and absor...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4306</td>\n",
       "      <td>5cecd1d9932d3f269b1caa45d07e84e3376c3710&gt;9a0f5...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>With respect to the contradicting results of o...</td>\n",
       "      <td>result</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4960</td>\n",
       "      <td>1890ccd4d2a4d542ba24081b593ab8060e38f101&gt;12732...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>We elected to take this course because formal ...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6921</td>\n",
       "      <td>e2e9ce75aab15707ff3b85378b61f161c0b7886a&gt;39688...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Nuclear localization of PTOV1 is required for ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3526</td>\n",
       "      <td>afe94f58975df56a9c0450c8153c2dd43d0b597b&gt;ec694...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The remaining medial portion of the bone was p...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID    source  \\\n",
       "0        3923  6828a9cc19290ccd8cc99ccc678a042b00d92125>26cad...  explicit   \n",
       "1        4306  5cecd1d9932d3f269b1caa45d07e84e3376c3710>9a0f5...  explicit   \n",
       "2        4960  1890ccd4d2a4d542ba24081b593ab8060e38f101>12732...  explicit   \n",
       "3        6921  e2e9ce75aab15707ff3b85378b61f161c0b7886a>39688...  explicit   \n",
       "4        3526  afe94f58975df56a9c0450c8153c2dd43d0b597b>ec694...  explicit   \n",
       "\n",
       "                                              string       label sentiment  \n",
       "0  The relationship between DNA content and absor...  background  positive  \n",
       "1  With respect to the contradicting results of o...      result  negative  \n",
       "2  We elected to take this course because formal ...      method   neutral  \n",
       "3  Nuclear localization of PTOV1 is required for ...  background   neutral  \n",
       "4  The remaining medial portion of the bone was p...      method   neutral  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in Daniel's subset (after annotating manually)\n",
    "d_df_annotated = pd.read_csv('sentimentAnnotations_CSV/annotated_D_1.csv')\n",
    "d_df_annotated.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b63ebd2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7792</th>\n",
       "      <td>0244de3610992df3e77b65de794861810a673a48&gt;89634...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The self-report component measures interpretat...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3733</th>\n",
       "      <td>f98fa4590950e29e660b3d3179a7bb570ee4e132&gt;cf4b7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The age of giant nupDNA fragment A was calcula...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1237</th>\n",
       "      <td>bc103d96366ec97e0dd620894bbb04c8849eb772&gt;919a7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two representative software of this type are s...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7052</th>\n",
       "      <td>e3450e8ca6d743f04a1ad41297ab59eeb82299b4&gt;b5fcd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>How are Bcl-2 and Mcl-1 levels regulated in a ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6524</th>\n",
       "      <td>8ebd0c64a3f1833cac7b8191303ea2bbd05682b5&gt;86fde...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two traits are orthogonal when, based on indiv...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID    source  \\\n",
       "7792  0244de3610992df3e77b65de794861810a673a48>89634...  explicit   \n",
       "3733  f98fa4590950e29e660b3d3179a7bb570ee4e132>cf4b7...  explicit   \n",
       "1237  bc103d96366ec97e0dd620894bbb04c8849eb772>919a7...  explicit   \n",
       "7052  e3450e8ca6d743f04a1ad41297ab59eeb82299b4>b5fcd...  explicit   \n",
       "6524  8ebd0c64a3f1833cac7b8191303ea2bbd05682b5>86fde...  explicit   \n",
       "\n",
       "                                                 string       label  \\\n",
       "7792  The self-report component measures interpretat...  background   \n",
       "3733  The age of giant nupDNA fragment A was calcula...      method   \n",
       "1237  Two representative software of this type are s...      method   \n",
       "7052  How are Bcl-2 and Mcl-1 levels regulated in a ...  background   \n",
       "6524  Two traits are orthogonal when, based on indiv...  background   \n",
       "\n",
       "     sentiment_chris sentiment_daniel  \n",
       "7792                                   \n",
       "3733                                   \n",
       "1237                                   \n",
       "7052                                   \n",
       "6524                                   "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Mutual subset in DataFrame form (for adding IAA sentiment annotations)\n",
    "m_df = train_tsv.copy()\n",
    "m_df = m_df.iloc[m_sample]\n",
    "m_df['sentiment_chris'] = ['']*25\n",
    "m_df['sentiment_daniel'] = ['']*25\n",
    "m_df.to_csv('sentimentAnnotations_CSV/blankAnnotations_M_1.csv')\n",
    "m_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6546f61f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7792</td>\n",
       "      <td>0244de3610992df3e77b65de794861810a673a48&gt;89634...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The self-report component measures interpretat...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3733</td>\n",
       "      <td>f98fa4590950e29e660b3d3179a7bb570ee4e132&gt;cf4b7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The age of giant nupDNA fragment A was calcula...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237</td>\n",
       "      <td>bc103d96366ec97e0dd620894bbb04c8849eb772&gt;919a7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two representative software of this type are s...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7052</td>\n",
       "      <td>e3450e8ca6d743f04a1ad41297ab59eeb82299b4&gt;b5fcd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>How are Bcl-2 and Mcl-1 levels regulated in a ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6524</td>\n",
       "      <td>8ebd0c64a3f1833cac7b8191303ea2bbd05682b5&gt;86fde...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two traits are orthogonal when, based on indiv...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID    source  \\\n",
       "0        7792  0244de3610992df3e77b65de794861810a673a48>89634...  explicit   \n",
       "1        3733  f98fa4590950e29e660b3d3179a7bb570ee4e132>cf4b7...  explicit   \n",
       "2        1237  bc103d96366ec97e0dd620894bbb04c8849eb772>919a7...  explicit   \n",
       "3        7052  e3450e8ca6d743f04a1ad41297ab59eeb82299b4>b5fcd...  explicit   \n",
       "4        6524  8ebd0c64a3f1833cac7b8191303ea2bbd05682b5>86fde...  explicit   \n",
       "\n",
       "                                              string       label  \\\n",
       "0  The self-report component measures interpretat...  background   \n",
       "1  The age of giant nupDNA fragment A was calcula...      method   \n",
       "2  Two representative software of this type are s...      method   \n",
       "3  How are Bcl-2 and Mcl-1 levels regulated in a ...  background   \n",
       "4  Two traits are orthogonal when, based on indiv...  background   \n",
       "\n",
       "  sentiment_chris sentiment_daniel  \n",
       "0         neutral          neutral  \n",
       "1         neutral          neutral  \n",
       "2         neutral          neutral  \n",
       "3         neutral          neutral  \n",
       "4         neutral          neutral  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in first mutual annotations subset\n",
    "m_df_annotated = pd.read_csv('sentimentAnnotations_CSV/annotated_M_1.csv')\n",
    "m_df_annotated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4086fdd",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Second set of sentiment annotations (Week of Jul. 4, 2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ec92feb8",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "### Second sets of random samples for sentiment annotations\n",
    "train_sentiment_2 = train_sentiment.copy()\n",
    "\n",
    "# Chris' second subset\n",
    "c_sample_2 = list(train_sentiment_2['string'].sample(n=100, random_state=1).index)\n",
    "# Drop\n",
    "train_sentiment_2 = train_sentiment_2.drop(c_sample_2)\n",
    "\n",
    "# Daniel's second subset\n",
    "d_sample_2 = list(train_sentiment_2['string'].sample(n=200, random_state=1).index)\n",
    "# Drop\n",
    "train_sentiment_2 = train_sentiment_2.drop(d_sample_2)\n",
    "\n",
    "# Second mutual subset (for IAA score)\n",
    "m_sample_2 = list(train_sentiment_2['string'].sample(n=25, random_state=1).index)\n",
    "train_sentiment_2 = train_sentiment_2.drop(m_sample_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ea490a67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3344</th>\n",
       "      <td>5fa7cd8dfc1907789705613dda8328f7f6ecfc53&gt;ad9a2...</td>\n",
       "      <td>properNoun</td>\n",
       "      <td>Cases were identified and clinical data collec...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5322</th>\n",
       "      <td>410376c6a35d3dd09a544e0b334ac4e11e22da52&gt;bc09d...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>[5,6,13,14] The abdominal superficial venous d...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5387</th>\n",
       "      <td>c20db8a8cd0649dd3644a897404c8d5100b7c707&gt;76ae7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>be involved in protein binding (especially of ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1793</th>\n",
       "      <td>e300bde47ff38b78bd09fc3566e9c3b5a0aa21e5&gt;6cd19...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>We began the analysis with open coding that in...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>776</th>\n",
       "      <td>ca4d2aef7421153f0f2c93db54a3e06a4942d329&gt;a0ecd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Figure 3 shows the fit of the model to the dyn...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID      source  \\\n",
       "3344  5fa7cd8dfc1907789705613dda8328f7f6ecfc53>ad9a2...  properNoun   \n",
       "5322  410376c6a35d3dd09a544e0b334ac4e11e22da52>bc09d...    explicit   \n",
       "5387  c20db8a8cd0649dd3644a897404c8d5100b7c707>76ae7...    explicit   \n",
       "1793  e300bde47ff38b78bd09fc3566e9c3b5a0aa21e5>6cd19...    explicit   \n",
       "776   ca4d2aef7421153f0f2c93db54a3e06a4942d329>a0ecd...    explicit   \n",
       "\n",
       "                                                 string       label sentiment  \n",
       "3344  Cases were identified and clinical data collec...      method            \n",
       "5322  [5,6,13,14] The abdominal superficial venous d...  background            \n",
       "5387  be involved in protein binding (especially of ...  background            \n",
       "1793  We began the analysis with open coding that in...      method            \n",
       "776   Figure 3 shows the fit of the model to the dyn...  background            "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chris' second subset in DataFrame form (for adding a sentiment column/feature)\n",
    "c_df_2 = train_tsv.copy()\n",
    "c_df_2 = c_df_2.iloc[c_sample_2]\n",
    "c_df_2['sentiment'] = ['']*100\n",
    "c_df_2.to_csv('sentimentAnnotations_CSV/blankAnnotations_C_2.csv')\n",
    "c_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "501ede67",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3344</td>\n",
       "      <td>5fa7cd8dfc1907789705613dda8328f7f6ecfc53&gt;ad9a2...</td>\n",
       "      <td>properNoun</td>\n",
       "      <td>Cases were identified and clinical data collec...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5322</td>\n",
       "      <td>410376c6a35d3dd09a544e0b334ac4e11e22da52&gt;bc09d...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>[5,6,13,14] The abdominal superficial venous d...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5387</td>\n",
       "      <td>c20db8a8cd0649dd3644a897404c8d5100b7c707&gt;76ae7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>be involved in protein binding (especially of ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1793</td>\n",
       "      <td>e300bde47ff38b78bd09fc3566e9c3b5a0aa21e5&gt;6cd19...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>We began the analysis with open coding that in...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>776</td>\n",
       "      <td>ca4d2aef7421153f0f2c93db54a3e06a4942d329&gt;a0ecd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Figure 3 shows the fit of the model to the dyn...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID      source  \\\n",
       "0        3344  5fa7cd8dfc1907789705613dda8328f7f6ecfc53>ad9a2...  properNoun   \n",
       "1        5322  410376c6a35d3dd09a544e0b334ac4e11e22da52>bc09d...    explicit   \n",
       "2        5387  c20db8a8cd0649dd3644a897404c8d5100b7c707>76ae7...    explicit   \n",
       "3        1793  e300bde47ff38b78bd09fc3566e9c3b5a0aa21e5>6cd19...    explicit   \n",
       "4         776  ca4d2aef7421153f0f2c93db54a3e06a4942d329>a0ecd...    explicit   \n",
       "\n",
       "                                              string       label sentiment  \n",
       "0  Cases were identified and clinical data collec...      method   neutral  \n",
       "1  [5,6,13,14] The abdominal superficial venous d...  background  positive  \n",
       "2  be involved in protein binding (especially of ...  background   neutral  \n",
       "3  We began the analysis with open coding that in...      method   neutral  \n",
       "4  Figure 3 shows the fit of the model to the dyn...  background   neutral  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in Chris' second subset (after annotating manually)\n",
    "    ###Uncomment lines below after creating your second annotated subset\n",
    "c_df_annotated_2 = pd.read_csv('sentimentAnnotations_CSV/annotated_C_2.csv')\n",
    "c_df_annotated_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ddc4825f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>108</th>\n",
       "      <td>44b5a8fde9f6c45652d642ac9d27ecb12d755ce7&gt;a790a...</td>\n",
       "      <td>properNoun</td>\n",
       "      <td>Among possible explanations is the contributio...</td>\n",
       "      <td>result</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3287</th>\n",
       "      <td>0e4bf03c0e7a18078edc742edf2f356c8b29aace&gt;ddd33...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The 21 RCTs [15-35] and 42 observational studi...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8116</th>\n",
       "      <td>2f82dfa009bbf2a400908101438bf9a18a87a166&gt;f3e01...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The change in the apparent diameter of the par...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>27db8d3a2c85cbc34deef51ce4cd850b431b4b34&gt;a81b3...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The potential for BCL-xL to modulate other cha...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2976</th>\n",
       "      <td>b14d9802f95d696cb7f976988cf874e8f1273749&gt;a4cb5...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Antidepressant-like effects were measured usin...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID      source  \\\n",
       "108   44b5a8fde9f6c45652d642ac9d27ecb12d755ce7>a790a...  properNoun   \n",
       "3287  0e4bf03c0e7a18078edc742edf2f356c8b29aace>ddd33...    explicit   \n",
       "8116  2f82dfa009bbf2a400908101438bf9a18a87a166>f3e01...    explicit   \n",
       "77    27db8d3a2c85cbc34deef51ce4cd850b431b4b34>a81b3...    explicit   \n",
       "2976  b14d9802f95d696cb7f976988cf874e8f1273749>a4cb5...    explicit   \n",
       "\n",
       "                                                 string       label sentiment  \n",
       "108   Among possible explanations is the contributio...      result            \n",
       "3287  The 21 RCTs [15-35] and 42 observational studi...  background            \n",
       "8116  The change in the apparent diameter of the par...      method            \n",
       "77    The potential for BCL-xL to modulate other cha...  background            \n",
       "2976  Antidepressant-like effects were measured usin...      method            "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Daniel's second subset in DataFrame form (for adding a sentiment column/feature)\n",
    "d_df_2 = train_tsv.copy()\n",
    "d_df_2 = d_df_2.iloc[d_sample_2]\n",
    "d_df_2['sentiment'] = ['']*200\n",
    "d_df_2.to_csv('sentimentAnnotations_CSV/blankAnnotations_D_2.csv')\n",
    "d_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "858079ca",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>108</td>\n",
       "      <td>44b5a8fde9f6c45652d642ac9d27ecb12d755ce7&gt;a790a...</td>\n",
       "      <td>properNoun</td>\n",
       "      <td>Among possible explanations is the contributio...</td>\n",
       "      <td>result</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3287</td>\n",
       "      <td>0e4bf03c0e7a18078edc742edf2f356c8b29aace&gt;ddd33...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The 21 RCTs [15-35] and 42 observational studi...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8116</td>\n",
       "      <td>2f82dfa009bbf2a400908101438bf9a18a87a166&gt;f3e01...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The change in the apparent diameter of the par...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>77</td>\n",
       "      <td>27db8d3a2c85cbc34deef51ce4cd850b431b4b34&gt;a81b3...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The potential for BCL-xL to modulate other cha...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2976</td>\n",
       "      <td>b14d9802f95d696cb7f976988cf874e8f1273749&gt;a4cb5...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Antidepressant-like effects were measured usin...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID      source  \\\n",
       "0         108  44b5a8fde9f6c45652d642ac9d27ecb12d755ce7>a790a...  properNoun   \n",
       "1        3287  0e4bf03c0e7a18078edc742edf2f356c8b29aace>ddd33...    explicit   \n",
       "2        8116  2f82dfa009bbf2a400908101438bf9a18a87a166>f3e01...    explicit   \n",
       "3          77  27db8d3a2c85cbc34deef51ce4cd850b431b4b34>a81b3...    explicit   \n",
       "4        2976  b14d9802f95d696cb7f976988cf874e8f1273749>a4cb5...    explicit   \n",
       "\n",
       "                                              string       label sentiment  \n",
       "0  Among possible explanations is the contributio...      result   neutral  \n",
       "1  The 21 RCTs [15-35] and 42 observational studi...  background   neutral  \n",
       "2  The change in the apparent diameter of the par...      method   neutral  \n",
       "3  The potential for BCL-xL to modulate other cha...  background   neutral  \n",
       "4  Antidepressant-like effects were measured usin...      method   neutral  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in Daniel's second subset (after annotating manually)\n",
    "d_df_annotated_2 = pd.read_csv('sentimentAnnotations_CSV/annotated_D_2.csv')\n",
    "d_df_annotated_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c8d75463",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7422</th>\n",
       "      <td>cf627c9411e3bf3e67f23df5d5741ed48490a152&gt;3d2c9...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Upon initial pulmonary infection, a subset of ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5404</th>\n",
       "      <td>29cd81b4d95c3c99d2d27629e2acf0f5e3c2ec17&gt;54996...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The model (Rohrer and Berresheim, 2006; Hofzum...</td>\n",
       "      <td>method</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6247</th>\n",
       "      <td>ed60fe5cf0b2947f812a3d19d533d0f644b67b0d&gt;5af4e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In the discussion of their paper, Hendriks et ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>ac5fb99eda6b95e9703bb3d93417df5f7373e662&gt;e9891...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A serotype-shift occurred over the years with ...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>313</th>\n",
       "      <td>90e3135b8b3ae3871c2d272a5971b703174bcd96&gt;466e2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>To date, these reactions have primarily been i...</td>\n",
       "      <td>background</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          citingPaperID    source  \\\n",
       "7422  cf627c9411e3bf3e67f23df5d5741ed48490a152>3d2c9...  explicit   \n",
       "5404  29cd81b4d95c3c99d2d27629e2acf0f5e3c2ec17>54996...  explicit   \n",
       "6247  ed60fe5cf0b2947f812a3d19d533d0f644b67b0d>5af4e...  explicit   \n",
       "42    ac5fb99eda6b95e9703bb3d93417df5f7373e662>e9891...  explicit   \n",
       "313   90e3135b8b3ae3871c2d272a5971b703174bcd96>466e2...  explicit   \n",
       "\n",
       "                                                 string       label  \\\n",
       "7422  Upon initial pulmonary infection, a subset of ...  background   \n",
       "5404  The model (Rohrer and Berresheim, 2006; Hofzum...      method   \n",
       "6247  In the discussion of their paper, Hendriks et ...  background   \n",
       "42    A serotype-shift occurred over the years with ...  background   \n",
       "313   To date, these reactions have primarily been i...  background   \n",
       "\n",
       "     sentiment_chris sentiment_daniel  \n",
       "7422                                   \n",
       "5404                                   \n",
       "6247                                   \n",
       "42                                     \n",
       "313                                    "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Second mutual subset in DataFrame form (for adding IAA sentiment annotations)\n",
    "m_df_2 = train_tsv.copy()\n",
    "m_df_2 = m_df_2.iloc[m_sample_2]\n",
    "m_df_2['sentiment_chris'] = ['']*25\n",
    "m_df_2['sentiment_daniel'] = ['']*25\n",
    "m_df_2.to_csv('sentimentAnnotations_CSV/blankAnnotations_M_2.csv')\n",
    "m_df_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a857d1b3",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7422</td>\n",
       "      <td>cf627c9411e3bf3e67f23df5d5741ed48490a152&gt;3d2c9...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Upon initial pulmonary infection, a subset of ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5404</td>\n",
       "      <td>29cd81b4d95c3c99d2d27629e2acf0f5e3c2ec17&gt;54996...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The model (Rohrer and Berresheim, 2006; Hofzum...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6247</td>\n",
       "      <td>ed60fe5cf0b2947f812a3d19d533d0f644b67b0d&gt;5af4e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In the discussion of their paper, Hendriks et ...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42</td>\n",
       "      <td>ac5fb99eda6b95e9703bb3d93417df5f7373e662&gt;e9891...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A serotype-shift occurred over the years with ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>313</td>\n",
       "      <td>90e3135b8b3ae3871c2d272a5971b703174bcd96&gt;466e2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>To date, these reactions have primarily been i...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                      citingPaperID    source  \\\n",
       "0        7422  cf627c9411e3bf3e67f23df5d5741ed48490a152>3d2c9...  explicit   \n",
       "1        5404  29cd81b4d95c3c99d2d27629e2acf0f5e3c2ec17>54996...  explicit   \n",
       "2        6247  ed60fe5cf0b2947f812a3d19d533d0f644b67b0d>5af4e...  explicit   \n",
       "3          42  ac5fb99eda6b95e9703bb3d93417df5f7373e662>e9891...  explicit   \n",
       "4         313  90e3135b8b3ae3871c2d272a5971b703174bcd96>466e2...  explicit   \n",
       "\n",
       "                                              string       label  \\\n",
       "0  Upon initial pulmonary infection, a subset of ...  background   \n",
       "1  The model (Rohrer and Berresheim, 2006; Hofzum...      method   \n",
       "2  In the discussion of their paper, Hendriks et ...  background   \n",
       "3  A serotype-shift occurred over the years with ...  background   \n",
       "4  To date, these reactions have primarily been i...  background   \n",
       "\n",
       "  sentiment_chris sentiment_daniel  \n",
       "0         neutral          neutral  \n",
       "1         neutral         positive  \n",
       "2        positive          neutral  \n",
       "3         neutral          neutral  \n",
       "4         neutral          neutral  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading in second mutual annotations subset\n",
    "m_df_annotated_2 = pd.read_csv('sentimentAnnotations_CSV/annotated_M_2.csv')\n",
    "m_df_annotated_2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b546b179",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8097"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More negative data\n",
    "negative = pd.read_csv('negative.csv')\n",
    "train_sentiment = train_sentiment.drop(list(negative.iloc[:, 0]))\n",
    "len(train_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20bb903",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Joining annotated data together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b40e82a",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "#annotated = pd.concat([c_df_annotated, d_df_annotated])\n",
    "    ### Comment above line and uncomment lines below after all annotations have been completed\n",
    "annotated = pd.concat([c_df_annotated, d_df_annotated, c_df_annotated_2, d_df_annotated_2, negative])\n",
    "annotated_mutual = pd.concat([m_df_annotated, m_df_annotated_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9edfa9f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "annotated.to_csv('sentimentAnnotations_CSV/combined_annotation.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "75126c12",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "annotated_mutual.to_csv(\"sentimentAnnotations_CSV/combined_annotation_mutual.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a311506",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating IAA score (Cohen's kappa) for first session\n",
    "mutual_chris = list(m_df_annotated['sentiment_chris'])\n",
    "mutual_daniel = list(m_df_annotated['sentiment_daniel'])\n",
    "\n",
    "p_o = 0\n",
    "for i in np.arange(25):\n",
    "    if mutual_chris[i] == mutual_daniel[i]:\n",
    "        p_o += 1\n",
    "p_o /= 25\n",
    "\n",
    "p_e = 0\n",
    "for value in ['positive', 'neutral', 'negative']:\n",
    "    p_sentiment = (mutual_chris.count(value)/25) * (mutual_daniel.count(value)/25)\n",
    "    p_e += p_sentiment\n",
    "\n",
    "kappa = (p_o - p_e) / (1 - p_e)\n",
    "kappa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3889c7",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating IAA score (Cohen's kappa) after second session\n",
    "mutual_chris = list(annotated_mutual['sentiment_chris'])\n",
    "mutual_daniel = list(annotated_mutual['sentiment_daniel'])\n",
    "\n",
    "p_o = 0\n",
    "for i in np.arange(50):\n",
    "    if mutual_chris[i] == mutual_daniel[i]:\n",
    "        p_o += 1\n",
    "p_o /= 50\n",
    "\n",
    "p_e = 0\n",
    "for value in ['positive', 'neutral', 'negative']:\n",
    "    p_sentiment = (mutual_chris.count(value)/50) * (mutual_daniel.count(value)/50)\n",
    "    p_e += p_sentiment\n",
    "\n",
    "kappa_final = (p_o - p_e) / (1 - p_e)\n",
    "kappa_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd7b2e0d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applying sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69908aae",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "361d34cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f13212de",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dff6fe",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m_df_annotated['pos'],m_df_annotated['neg'],m_df_annotated['neu'] = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc08759c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a=m_df_annotated['string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "865643cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output = sid.polarity_scores(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455263c4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Calculating final IAA score (Cohen's kappa)\n",
    "mutual_chris = list(annotated_mutual['sentiment_chris'])\n",
    "mutual_daniel = list(annotated_mutual['sentiment_daniel'])\n",
    "\n",
    "p_o = 0\n",
    "for i in np.arange(50):\n",
    "    if mutual_chris[i] == mutual_daniel[i]:\n",
    "        p_o += 1\n",
    "p_o /= 50\n",
    "\n",
    "p_e = 0\n",
    "for value in ['positive', 'neutral', 'negative']:\n",
    "    p_sentiment = (mutual_chris.count(value)/50) * (mutual_daniel.count(value)/50)\n",
    "    p_e += p_sentiment\n",
    "\n",
    "kappa_final = (p_o - p_e) / (1 - p_e)\n",
    "kappa_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c1a7df5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7330960854092526"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating final IAA score (Cohen's kappa)\n",
    "mutual_chris = list(annotated_mutual['sentiment_chris'])\n",
    "mutual_daniel = list(annotated_mutual['sentiment_daniel'])\n",
    "\n",
    "p_o = 0\n",
    "for i in np.arange(50):\n",
    "    if mutual_chris[i] == mutual_daniel[i]:\n",
    "        p_o += 1\n",
    "p_o /= 50\n",
    "\n",
    "p_e = 0\n",
    "for value in ['positive', 'neutral', 'negative']:\n",
    "    p_sentiment = (mutual_chris.count(value)/50) * (mutual_daniel.count(value)/50)\n",
    "    p_e += p_sentiment\n",
    "\n",
    "kappa_final = (p_o - p_e) / (1 - p_e)\n",
    "kappa_final"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef43a17",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Applying sentiment classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4fa50c25",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87a3fdb2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/lolai/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b53d2516",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa44a2ff",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m_df_annotated['pos'],m_df_annotated['neg'],m_df_annotated['neu'] = 0,0,0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b6b37226",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "a=m_df_annotated['string'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "310e3301",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "output = sid.polarity_scores(\"Hello there\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "006eeed1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m_df_annotated['sc ores'] = m_df_annotated['string'].apply(lambda string: sid.polarity_scores(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "755d2c4b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "      <th>sc ores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7792</td>\n",
       "      <td>0244de3610992df3e77b65de794861810a673a48&gt;89634...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The self-report component measures interpretat...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.176, 'neu': 0.621, 'pos': 0.203, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3733</td>\n",
       "      <td>f98fa4590950e29e660b3d3179a7bb570ee4e132&gt;cf4b7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The age of giant nupDNA fragment A was calcula...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237</td>\n",
       "      <td>bc103d96366ec97e0dd620894bbb04c8849eb772&gt;919a7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two representative software of this type are s...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7052</td>\n",
       "      <td>e3450e8ca6d743f04a1ad41297ab59eeb82299b4&gt;b5fcd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>How are Bcl-2 and Mcl-1 levels regulated in a ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6524</td>\n",
       "      <td>8ebd0c64a3f1833cac7b8191303ea2bbd05682b5&gt;86fde...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two traits are orthogonal when, based on indiv...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2543</td>\n",
       "      <td>2c0cab0d657f6c225c74736fed02750c381032de&gt;02cdb...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>These results are similar to other reports [34...</td>\n",
       "      <td>result</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.059, 'neu': 0.88, 'pos': 0.062, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8093</td>\n",
       "      <td>7dda92d7dbb1a5f7fc78bba6f81890b3832cb2d6&gt;e6f6f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A type I T-cell gene signature that promotes b...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.131, 'neu': 0.73, 'pos': 0.139, 'com...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5365</td>\n",
       "      <td>9237db18530d0de2e0281e735c10570d7c235b36&gt;14682...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Resistance to TB involves macrophages [2,4], d...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4663</td>\n",
       "      <td>1ac9706400e1c06a100add4b83a67af503404d8d&gt;1fc59...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>While the prevalence of hyperechogenicity is o...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3065</td>\n",
       "      <td>4f4c6a035c0c28cfe5855a720a8f7a16a401e9f8&gt;0451f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>There have been few animal studies on this sub...</td>\n",
       "      <td>background</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>372</td>\n",
       "      <td>0fd548a07b8d113f5f6a6255abe191668120ac53&gt;74809...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Although Arpp/Ankrd2 has been reported to be l...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6346</td>\n",
       "      <td>4289898ef10c52412dd2ee16e35ae8ae4954552f&gt;2b2ac...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>memory\\nINTRODUCTION\\nEmerging studies have s...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.036, 'neu': 0.964, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2343</td>\n",
       "      <td>70b1e174c0d26a6262355e3891c2655559f79122&gt;45e3f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The prognosis of FVPTC in our study was excell...</td>\n",
       "      <td>result</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>600</td>\n",
       "      <td>9a068a63f5ae85227863a972c7204903fdcca978&gt;73619...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In addition to expression in the fetus, Igf2 i...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3791</td>\n",
       "      <td>af794c74a2bfd33336bca2a410ed42a978d98a89&gt;69157...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Other studies suggest that altering affinity t...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6315</td>\n",
       "      <td>ae1a3173131628123ac7c7579045aa96006c9485&gt;6220e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Although our cohort also consisted of treatmen...</td>\n",
       "      <td>result</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7737</td>\n",
       "      <td>d388588cbc8aef4f1eba7b56f1571191370f6e75&gt;7348e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>These study areas have all been managed in the...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7162</td>\n",
       "      <td>e9a4a6cec8b4a268ecd991a9f92700ce1a8960a5&gt;3ef88...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Assessing Risk of Bias\\nCritical appraisal of ...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.122, 'neu': 0.783, 'pos': 0.096, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3758</td>\n",
       "      <td>ea5e02b640e8b354ca4c997f2358a1da8837fc19&gt;2f0f0...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A retrospective cohort study abstracted medica...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8208</td>\n",
       "      <td>b4bafb8c1ff27c29f0ecef6caa775a0f2d86db00&gt;79b28...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Cancer cells usually have a different genome s...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.15, 'neu': 0.85, 'pos': 0.0, 'compou...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2397</td>\n",
       "      <td>387a4968c48a7f86693bcc455beffa97943cd445&gt;3fdc8...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>%) was a close relative (at 97% identity) of t...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.041, 'neu': 0.959, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2542</td>\n",
       "      <td>4ba2e2317d3211666340f58f90efc051a3503bef&gt;2a8c2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Moreover, phytochelatins have been found to be...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.177, 'neu': 0.679, 'pos': 0.144, 'co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4697</td>\n",
       "      <td>743a1befaf5e2cd2e5a9162432909b221a1e695c&gt;8bd0c...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>One of the key concerns by induction of reprog...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3954</td>\n",
       "      <td>37e4db73f2c3fa01c121afef0a938656bc03cebd&gt;447b2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In [1] and [2], a multilayer directory structu...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5098</td>\n",
       "      <td>eafafc708296c462edc9294e1b8f3b1dd3f4dffc&gt;8f125...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>difference between aspirated and lenis is mer...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.952, 'pos': 0.0, 'comp...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                      citingPaperID    source  \\\n",
       "0         7792  0244de3610992df3e77b65de794861810a673a48>89634...  explicit   \n",
       "1         3733  f98fa4590950e29e660b3d3179a7bb570ee4e132>cf4b7...  explicit   \n",
       "2         1237  bc103d96366ec97e0dd620894bbb04c8849eb772>919a7...  explicit   \n",
       "3         7052  e3450e8ca6d743f04a1ad41297ab59eeb82299b4>b5fcd...  explicit   \n",
       "4         6524  8ebd0c64a3f1833cac7b8191303ea2bbd05682b5>86fde...  explicit   \n",
       "5         2543  2c0cab0d657f6c225c74736fed02750c381032de>02cdb...  explicit   \n",
       "6         8093  7dda92d7dbb1a5f7fc78bba6f81890b3832cb2d6>e6f6f...  explicit   \n",
       "7         5365  9237db18530d0de2e0281e735c10570d7c235b36>14682...  explicit   \n",
       "8         4663  1ac9706400e1c06a100add4b83a67af503404d8d>1fc59...  explicit   \n",
       "9         3065  4f4c6a035c0c28cfe5855a720a8f7a16a401e9f8>0451f...  explicit   \n",
       "10         372  0fd548a07b8d113f5f6a6255abe191668120ac53>74809...  explicit   \n",
       "11        6346  4289898ef10c52412dd2ee16e35ae8ae4954552f>2b2ac...  explicit   \n",
       "12        2343  70b1e174c0d26a6262355e3891c2655559f79122>45e3f...  explicit   \n",
       "13         600  9a068a63f5ae85227863a972c7204903fdcca978>73619...  explicit   \n",
       "14        3791  af794c74a2bfd33336bca2a410ed42a978d98a89>69157...  explicit   \n",
       "15        6315  ae1a3173131628123ac7c7579045aa96006c9485>6220e...  explicit   \n",
       "16        7737  d388588cbc8aef4f1eba7b56f1571191370f6e75>7348e...  explicit   \n",
       "17        7162  e9a4a6cec8b4a268ecd991a9f92700ce1a8960a5>3ef88...  explicit   \n",
       "18        3758  ea5e02b640e8b354ca4c997f2358a1da8837fc19>2f0f0...  explicit   \n",
       "19        8208  b4bafb8c1ff27c29f0ecef6caa775a0f2d86db00>79b28...  explicit   \n",
       "20        2397  387a4968c48a7f86693bcc455beffa97943cd445>3fdc8...  explicit   \n",
       "21        2542  4ba2e2317d3211666340f58f90efc051a3503bef>2a8c2...  explicit   \n",
       "22        4697  743a1befaf5e2cd2e5a9162432909b221a1e695c>8bd0c...  explicit   \n",
       "23        3954  37e4db73f2c3fa01c121afef0a938656bc03cebd>447b2...  explicit   \n",
       "24        5098  eafafc708296c462edc9294e1b8f3b1dd3f4dffc>8f125...  explicit   \n",
       "\n",
       "                                               string       label  \\\n",
       "0   The self-report component measures interpretat...  background   \n",
       "1   The age of giant nupDNA fragment A was calcula...      method   \n",
       "2   Two representative software of this type are s...      method   \n",
       "3   How are Bcl-2 and Mcl-1 levels regulated in a ...  background   \n",
       "4   Two traits are orthogonal when, based on indiv...  background   \n",
       "5   These results are similar to other reports [34...      result   \n",
       "6   A type I T-cell gene signature that promotes b...  background   \n",
       "7   Resistance to TB involves macrophages [2,4], d...  background   \n",
       "8   While the prevalence of hyperechogenicity is o...  background   \n",
       "9   There have been few animal studies on this sub...  background   \n",
       "10  Although Arpp/Ankrd2 has been reported to be l...  background   \n",
       "11  memory\\nINTRODUCTION\\nEmerging studies have s...  background   \n",
       "12  The prognosis of FVPTC in our study was excell...      result   \n",
       "13  In addition to expression in the fetus, Igf2 i...  background   \n",
       "14  Other studies suggest that altering affinity t...  background   \n",
       "15  Although our cohort also consisted of treatmen...      result   \n",
       "16  These study areas have all been managed in the...  background   \n",
       "17  Assessing Risk of Bias\\nCritical appraisal of ...      method   \n",
       "18  A retrospective cohort study abstracted medica...  background   \n",
       "19  Cancer cells usually have a different genome s...  background   \n",
       "20  %) was a close relative (at 97% identity) of t...  background   \n",
       "21  Moreover, phytochelatins have been found to be...  background   \n",
       "22  One of the key concerns by induction of reprog...  background   \n",
       "23  In [1] and [2], a multilayer directory structu...  background   \n",
       "24  difference between aspirated and lenis is mer...  background   \n",
       "\n",
       "   sentiment_chris sentiment_daniel  \\\n",
       "0          neutral          neutral   \n",
       "1          neutral          neutral   \n",
       "2          neutral          neutral   \n",
       "3          neutral          neutral   \n",
       "4          neutral          neutral   \n",
       "5         positive         positive   \n",
       "6         positive         positive   \n",
       "7         positive         positive   \n",
       "8         positive          neutral   \n",
       "9         negative         negative   \n",
       "10        positive         positive   \n",
       "11        positive         positive   \n",
       "12        positive         positive   \n",
       "13         neutral          neutral   \n",
       "14        positive          neutral   \n",
       "15        negative         negative   \n",
       "16         neutral          neutral   \n",
       "17         neutral          neutral   \n",
       "18         neutral          neutral   \n",
       "19         neutral          neutral   \n",
       "20         neutral          neutral   \n",
       "21        positive          neutral   \n",
       "22         neutral          neutral   \n",
       "23         neutral         positive   \n",
       "24         neutral          neutral   \n",
       "\n",
       "                                              sc ores  \n",
       "0   {'neg': 0.176, 'neu': 0.621, 'pos': 0.203, 'co...  \n",
       "1   {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'comp...  \n",
       "2   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "3   {'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'comp...  \n",
       "4   {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp...  \n",
       "5   {'neg': 0.059, 'neu': 0.88, 'pos': 0.062, 'com...  \n",
       "6   {'neg': 0.131, 'neu': 0.73, 'pos': 0.139, 'com...  \n",
       "7   {'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'comp...  \n",
       "8   {'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compou...  \n",
       "9   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "10  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "11  {'neg': 0.036, 'neu': 0.964, 'pos': 0.0, 'comp...  \n",
       "12  {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'comp...  \n",
       "13  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "14  {'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'comp...  \n",
       "15  {'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'comp...  \n",
       "16  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "17  {'neg': 0.122, 'neu': 0.783, 'pos': 0.096, 'co...  \n",
       "18  {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...  \n",
       "19  {'neg': 0.15, 'neu': 0.85, 'pos': 0.0, 'compou...  \n",
       "20  {'neg': 0.041, 'neu': 0.959, 'pos': 0.0, 'comp...  \n",
       "21  {'neg': 0.177, 'neu': 0.679, 'pos': 0.144, 'co...  \n",
       "22  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  \n",
       "23  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...  \n",
       "24  {'neg': 0.048, 'neu': 0.952, 'pos': 0.0, 'comp...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "75655e55",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     /home/lolai/nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/lolai/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob.sentiments import NaiveBayesAnalyzer\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "554970a5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "for index, row in m_df_annotated.iterrows():\n",
    "    blob = TextBlob(row['string'], analyzer=NaiveBayesAnalyzer())\n",
    "    m_df_annotated.at[index, 'blob'] = blob.sentiment[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "353f18f9",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "m_df_annotated['blob'] = m_df_annotated['string'].apply(lambda string: TextBlob(string).polarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "73c035e0",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>citingPaperID</th>\n",
       "      <th>source</th>\n",
       "      <th>string</th>\n",
       "      <th>label</th>\n",
       "      <th>sentiment_chris</th>\n",
       "      <th>sentiment_daniel</th>\n",
       "      <th>sc ores</th>\n",
       "      <th>blob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7792</td>\n",
       "      <td>0244de3610992df3e77b65de794861810a673a48&gt;89634...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The self-report component measures interpretat...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.176, 'neu': 0.621, 'pos': 0.203, 'co...</td>\n",
       "      <td>-0.036364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3733</td>\n",
       "      <td>f98fa4590950e29e660b3d3179a7bb570ee4e132&gt;cf4b7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The age of giant nupDNA fragment A was calcula...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'comp...</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1237</td>\n",
       "      <td>bc103d96366ec97e0dd620894bbb04c8849eb772&gt;919a7...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two representative software of this type are s...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7052</td>\n",
       "      <td>e3450e8ca6d743f04a1ad41297ab59eeb82299b4&gt;b5fcd...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>How are Bcl-2 and Mcl-1 levels regulated in a ...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'comp...</td>\n",
       "      <td>-0.051852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6524</td>\n",
       "      <td>8ebd0c64a3f1833cac7b8191303ea2bbd05682b5&gt;86fde...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Two traits are orthogonal when, based on indiv...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp...</td>\n",
       "      <td>-0.144345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2543</td>\n",
       "      <td>2c0cab0d657f6c225c74736fed02750c381032de&gt;02cdb...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>These results are similar to other reports [34...</td>\n",
       "      <td>result</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.059, 'neu': 0.88, 'pos': 0.062, 'com...</td>\n",
       "      <td>-0.062500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8093</td>\n",
       "      <td>7dda92d7dbb1a5f7fc78bba6f81890b3832cb2d6&gt;e6f6f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A type I T-cell gene signature that promotes b...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.131, 'neu': 0.73, 'pos': 0.139, 'com...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5365</td>\n",
       "      <td>9237db18530d0de2e0281e735c10570d7c235b36&gt;14682...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Resistance to TB involves macrophages [2,4], d...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4663</td>\n",
       "      <td>1ac9706400e1c06a100add4b83a67af503404d8d&gt;1fc59...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>While the prevalence of hyperechogenicity is o...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compou...</td>\n",
       "      <td>0.343750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3065</td>\n",
       "      <td>4f4c6a035c0c28cfe5855a720a8f7a16a401e9f8&gt;0451f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>There have been few animal studies on this sub...</td>\n",
       "      <td>background</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>-0.183333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>372</td>\n",
       "      <td>0fd548a07b8d113f5f6a6255abe191668120ac53&gt;74809...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Although Arpp/Ankrd2 has been reported to be l...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.083333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>6346</td>\n",
       "      <td>4289898ef10c52412dd2ee16e35ae8ae4954552f&gt;2b2ac...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>memory\\nINTRODUCTION\\nEmerging studies have s...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.036, 'neu': 0.964, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2343</td>\n",
       "      <td>70b1e174c0d26a6262355e3891c2655559f79122&gt;45e3f...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>The prognosis of FVPTC in our study was excell...</td>\n",
       "      <td>result</td>\n",
       "      <td>positive</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'comp...</td>\n",
       "      <td>0.103968</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>600</td>\n",
       "      <td>9a068a63f5ae85227863a972c7204903fdcca978&gt;73619...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In addition to expression in the fetus, Igf2 i...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>-0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>3791</td>\n",
       "      <td>af794c74a2bfd33336bca2a410ed42a978d98a89&gt;69157...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Other studies suggest that altering affinity t...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.012500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>6315</td>\n",
       "      <td>ae1a3173131628123ac7c7579045aa96006c9485&gt;6220e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Although our cohort also consisted of treatmen...</td>\n",
       "      <td>result</td>\n",
       "      <td>negative</td>\n",
       "      <td>negative</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'comp...</td>\n",
       "      <td>0.069444</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>7737</td>\n",
       "      <td>d388588cbc8aef4f1eba7b56f1571191370f6e75&gt;7348e...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>These study areas have all been managed in the...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>-0.092500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7162</td>\n",
       "      <td>e9a4a6cec8b4a268ecd991a9f92700ce1a8960a5&gt;3ef88...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Assessing Risk of Bias\\nCritical appraisal of ...</td>\n",
       "      <td>method</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.122, 'neu': 0.783, 'pos': 0.096, 'co...</td>\n",
       "      <td>0.066667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>3758</td>\n",
       "      <td>ea5e02b640e8b354ca4c997f2358a1da8837fc19&gt;2f0f0...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>A retrospective cohort study abstracted medica...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>8208</td>\n",
       "      <td>b4bafb8c1ff27c29f0ecef6caa775a0f2d86db00&gt;79b28...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Cancer cells usually have a different genome s...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.15, 'neu': 0.85, 'pos': 0.0, 'compou...</td>\n",
       "      <td>-0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2397</td>\n",
       "      <td>387a4968c48a7f86693bcc455beffa97943cd445&gt;3fdc8...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>%) was a close relative (at 97% identity) of t...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.041, 'neu': 0.959, 'pos': 0.0, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2542</td>\n",
       "      <td>4ba2e2317d3211666340f58f90efc051a3503bef&gt;2a8c2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>Moreover, phytochelatins have been found to be...</td>\n",
       "      <td>background</td>\n",
       "      <td>positive</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.177, 'neu': 0.679, 'pos': 0.144, 'co...</td>\n",
       "      <td>0.550000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4697</td>\n",
       "      <td>743a1befaf5e2cd2e5a9162432909b221a1e695c&gt;8bd0c...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>One of the key concerns by induction of reprog...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>3954</td>\n",
       "      <td>37e4db73f2c3fa01c121afef0a938656bc03cebd&gt;447b2...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>In [1] and [2], a multilayer directory structu...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>positive</td>\n",
       "      <td>{'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>5098</td>\n",
       "      <td>eafafc708296c462edc9294e1b8f3b1dd3f4dffc&gt;8f125...</td>\n",
       "      <td>explicit</td>\n",
       "      <td>difference between aspirated and lenis is mer...</td>\n",
       "      <td>background</td>\n",
       "      <td>neutral</td>\n",
       "      <td>neutral</td>\n",
       "      <td>{'neg': 0.048, 'neu': 0.952, 'pos': 0.0, 'comp...</td>\n",
       "      <td>-0.095833</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                                      citingPaperID    source  \\\n",
       "0         7792  0244de3610992df3e77b65de794861810a673a48>89634...  explicit   \n",
       "1         3733  f98fa4590950e29e660b3d3179a7bb570ee4e132>cf4b7...  explicit   \n",
       "2         1237  bc103d96366ec97e0dd620894bbb04c8849eb772>919a7...  explicit   \n",
       "3         7052  e3450e8ca6d743f04a1ad41297ab59eeb82299b4>b5fcd...  explicit   \n",
       "4         6524  8ebd0c64a3f1833cac7b8191303ea2bbd05682b5>86fde...  explicit   \n",
       "5         2543  2c0cab0d657f6c225c74736fed02750c381032de>02cdb...  explicit   \n",
       "6         8093  7dda92d7dbb1a5f7fc78bba6f81890b3832cb2d6>e6f6f...  explicit   \n",
       "7         5365  9237db18530d0de2e0281e735c10570d7c235b36>14682...  explicit   \n",
       "8         4663  1ac9706400e1c06a100add4b83a67af503404d8d>1fc59...  explicit   \n",
       "9         3065  4f4c6a035c0c28cfe5855a720a8f7a16a401e9f8>0451f...  explicit   \n",
       "10         372  0fd548a07b8d113f5f6a6255abe191668120ac53>74809...  explicit   \n",
       "11        6346  4289898ef10c52412dd2ee16e35ae8ae4954552f>2b2ac...  explicit   \n",
       "12        2343  70b1e174c0d26a6262355e3891c2655559f79122>45e3f...  explicit   \n",
       "13         600  9a068a63f5ae85227863a972c7204903fdcca978>73619...  explicit   \n",
       "14        3791  af794c74a2bfd33336bca2a410ed42a978d98a89>69157...  explicit   \n",
       "15        6315  ae1a3173131628123ac7c7579045aa96006c9485>6220e...  explicit   \n",
       "16        7737  d388588cbc8aef4f1eba7b56f1571191370f6e75>7348e...  explicit   \n",
       "17        7162  e9a4a6cec8b4a268ecd991a9f92700ce1a8960a5>3ef88...  explicit   \n",
       "18        3758  ea5e02b640e8b354ca4c997f2358a1da8837fc19>2f0f0...  explicit   \n",
       "19        8208  b4bafb8c1ff27c29f0ecef6caa775a0f2d86db00>79b28...  explicit   \n",
       "20        2397  387a4968c48a7f86693bcc455beffa97943cd445>3fdc8...  explicit   \n",
       "21        2542  4ba2e2317d3211666340f58f90efc051a3503bef>2a8c2...  explicit   \n",
       "22        4697  743a1befaf5e2cd2e5a9162432909b221a1e695c>8bd0c...  explicit   \n",
       "23        3954  37e4db73f2c3fa01c121afef0a938656bc03cebd>447b2...  explicit   \n",
       "24        5098  eafafc708296c462edc9294e1b8f3b1dd3f4dffc>8f125...  explicit   \n",
       "\n",
       "                                               string       label  \\\n",
       "0   The self-report component measures interpretat...  background   \n",
       "1   The age of giant nupDNA fragment A was calcula...      method   \n",
       "2   Two representative software of this type are s...      method   \n",
       "3   How are Bcl-2 and Mcl-1 levels regulated in a ...  background   \n",
       "4   Two traits are orthogonal when, based on indiv...  background   \n",
       "5   These results are similar to other reports [34...      result   \n",
       "6   A type I T-cell gene signature that promotes b...  background   \n",
       "7   Resistance to TB involves macrophages [2,4], d...  background   \n",
       "8   While the prevalence of hyperechogenicity is o...  background   \n",
       "9   There have been few animal studies on this sub...  background   \n",
       "10  Although Arpp/Ankrd2 has been reported to be l...  background   \n",
       "11  memory\\nINTRODUCTION\\nEmerging studies have s...  background   \n",
       "12  The prognosis of FVPTC in our study was excell...      result   \n",
       "13  In addition to expression in the fetus, Igf2 i...  background   \n",
       "14  Other studies suggest that altering affinity t...  background   \n",
       "15  Although our cohort also consisted of treatmen...      result   \n",
       "16  These study areas have all been managed in the...  background   \n",
       "17  Assessing Risk of Bias\\nCritical appraisal of ...      method   \n",
       "18  A retrospective cohort study abstracted medica...  background   \n",
       "19  Cancer cells usually have a different genome s...  background   \n",
       "20  %) was a close relative (at 97% identity) of t...  background   \n",
       "21  Moreover, phytochelatins have been found to be...  background   \n",
       "22  One of the key concerns by induction of reprog...  background   \n",
       "23  In [1] and [2], a multilayer directory structu...  background   \n",
       "24  difference between aspirated and lenis is mer...  background   \n",
       "\n",
       "   sentiment_chris sentiment_daniel  \\\n",
       "0          neutral          neutral   \n",
       "1          neutral          neutral   \n",
       "2          neutral          neutral   \n",
       "3          neutral          neutral   \n",
       "4          neutral          neutral   \n",
       "5         positive         positive   \n",
       "6         positive         positive   \n",
       "7         positive         positive   \n",
       "8         positive          neutral   \n",
       "9         negative         negative   \n",
       "10        positive         positive   \n",
       "11        positive         positive   \n",
       "12        positive         positive   \n",
       "13         neutral          neutral   \n",
       "14        positive          neutral   \n",
       "15        negative         negative   \n",
       "16         neutral          neutral   \n",
       "17         neutral          neutral   \n",
       "18         neutral          neutral   \n",
       "19         neutral          neutral   \n",
       "20         neutral          neutral   \n",
       "21        positive          neutral   \n",
       "22         neutral          neutral   \n",
       "23         neutral         positive   \n",
       "24         neutral          neutral   \n",
       "\n",
       "                                              sc ores      blob  \n",
       "0   {'neg': 0.176, 'neu': 0.621, 'pos': 0.203, 'co... -0.036364  \n",
       "1   {'neg': 0.0, 'neu': 0.938, 'pos': 0.062, 'comp...  0.100000  \n",
       "2   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  0.000000  \n",
       "3   {'neg': 0.0, 'neu': 0.931, 'pos': 0.069, 'comp... -0.051852  \n",
       "4   {'neg': 0.0, 'neu': 0.935, 'pos': 0.065, 'comp... -0.144345  \n",
       "5   {'neg': 0.059, 'neu': 0.88, 'pos': 0.062, 'com... -0.062500  \n",
       "6   {'neg': 0.131, 'neu': 0.73, 'pos': 0.139, 'com...  0.000000  \n",
       "7   {'neg': 0.0, 'neu': 0.948, 'pos': 0.052, 'comp...  0.000000  \n",
       "8   {'neg': 0.0, 'neu': 0.87, 'pos': 0.13, 'compou...  0.343750  \n",
       "9   {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... -0.183333  \n",
       "10  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  0.083333  \n",
       "11  {'neg': 0.036, 'neu': 0.964, 'pos': 0.0, 'comp...  0.000000  \n",
       "12  {'neg': 0.0, 'neu': 0.927, 'pos': 0.073, 'comp...  0.103968  \n",
       "13  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... -0.250000  \n",
       "14  {'neg': 0.055, 'neu': 0.945, 'pos': 0.0, 'comp... -0.012500  \n",
       "15  {'neg': 0.0, 'neu': 0.945, 'pos': 0.055, 'comp...  0.069444  \n",
       "16  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound... -0.092500  \n",
       "17  {'neg': 0.122, 'neu': 0.783, 'pos': 0.096, 'co...  0.066667  \n",
       "18  {'neg': 0.0, 'neu': 0.833, 'pos': 0.167, 'comp...  0.000000  \n",
       "19  {'neg': 0.15, 'neu': 0.85, 'pos': 0.0, 'compou... -0.020000  \n",
       "20  {'neg': 0.041, 'neu': 0.959, 'pos': 0.0, 'comp...  0.000000  \n",
       "21  {'neg': 0.177, 'neu': 0.679, 'pos': 0.144, 'co...  0.550000  \n",
       "22  {'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound...  0.300000  \n",
       "23  {'neg': 0.0, 'neu': 0.906, 'pos': 0.094, 'comp...  0.000000  \n",
       "24  {'neg': 0.048, 'neu': 0.952, 'pos': 0.0, 'comp... -0.095833  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m_df_annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64378c79",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train sentiment classifier on annotated data BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2a6f561c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments, AutoTokenizer, DataCollatorWithPadding\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56a25351",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/lolai/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "899ecbdad2f44bb2a6098db88732917b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    unsupervised: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 50000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb = datasets.load_dataset(\"imdb\")\n",
    "imdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29da4d5e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached shuffled indices for dataset at /home/lolai/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-8a9e43a6ac4acdff.arrow\n",
      "Loading cached shuffled indices for dataset at /home/lolai/.cache/huggingface/datasets/imdb/plain_text/1.0.0/2fdd8b9bcadd6e7055e742a706876ba43f19faee861df134affd7a3f60fc38a1/cache-2eff9f118d84c6fe.arrow\n"
     ]
    }
   ],
   "source": [
    "small_train_dataset = imdb[\"train\"].shuffle(seed=42).select([i for i in list(range(3000))])\n",
    "small_test_dataset = imdb[\"test\"].shuffle(seed=42).select([i for i in list(range(300))])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4b3af832",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Utilizing gain-offunction assays, we demonstra...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>maticity, refractoriness and conduction of the...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>For reovirus 1/L-induced ARDS, at day 9 postin...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Besides that, no ameliorate impacts have been ...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>However, limited information is available abou...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>A recent review by Kazan et al depicted curren...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>is variation among ecosystems, the most commo...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Thirdly, ErbB-3 is characterized by a defectiv...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>These clusters of informative voxels, along wi...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Combined arterial and venous thromboembolism w...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows  2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     label\n",
       "0   Utilizing gain-offunction assays, we demonstra...  positive\n",
       "1   maticity, refractoriness and conduction of the...   neutral\n",
       "2   For reovirus 1/L-induced ARDS, at day 9 postin...   neutral\n",
       "3   Besides that, no ameliorate impacts have been ...  negative\n",
       "4   However, limited information is available abou...  negative\n",
       "..                                                ...       ...\n",
       "45  A recent review by Kazan et al depicted curren...  positive\n",
       "46  is variation among ecosystems, the most commo...  positive\n",
       "47  Thirdly, ErbB-3 is characterized by a defectiv...   neutral\n",
       "48  These clusters of informative voxels, along wi...  positive\n",
       "49  Combined arterial and venous thromboembolism w...  positive\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pd = pd.DataFrame()\n",
    "bert_pd['text'], bert_pd['label'] = annotated['string'], annotated['sentiment']\n",
    "bert_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ad19a60b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The self-report component measures interpretat...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The age of giant nupDNA fragment A was calcula...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Two representative software of this type are s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How are Bcl-2 and Mcl-1 levels regulated in a ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Two traits are orthogonal when, based on indiv...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>These results are similar to other reports [34...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>A type I T-cell gene signature that promotes b...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Resistance to TB involves macrophages [2,4], d...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>While the prevalence of hyperechogenicity is o...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>There have been few animal studies on this sub...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Although Arpp/Ankrd2 has been reported to be l...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>memory\\nINTRODUCTION\\nEmerging studies have s...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The prognosis of FVPTC in our study was excell...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>In addition to expression in the fetus, Igf2 i...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Other studies suggest that altering affinity t...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Although our cohort also consisted of treatmen...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>These study areas have all been managed in the...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Assessing Risk of Bias\\nCritical appraisal of ...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>A retrospective cohort study abstracted medica...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Cancer cells usually have a different genome s...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>%) was a close relative (at 97% identity) of t...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Moreover, phytochelatins have been found to be...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>One of the key concerns by induction of reprog...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>In [1] and [2], a multilayer directory structu...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>difference between aspirated and lenis is mer...</td>\n",
       "      <td>neutral</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text     label\n",
       "0   The self-report component measures interpretat...   neutral\n",
       "1   The age of giant nupDNA fragment A was calcula...   neutral\n",
       "2   Two representative software of this type are s...   neutral\n",
       "3   How are Bcl-2 and Mcl-1 levels regulated in a ...   neutral\n",
       "4   Two traits are orthogonal when, based on indiv...   neutral\n",
       "5   These results are similar to other reports [34...  positive\n",
       "6   A type I T-cell gene signature that promotes b...  positive\n",
       "7   Resistance to TB involves macrophages [2,4], d...  positive\n",
       "8   While the prevalence of hyperechogenicity is o...  positive\n",
       "9   There have been few animal studies on this sub...  negative\n",
       "10  Although Arpp/Ankrd2 has been reported to be l...  positive\n",
       "11  memory\\nINTRODUCTION\\nEmerging studies have s...  positive\n",
       "12  The prognosis of FVPTC in our study was excell...  positive\n",
       "13  In addition to expression in the fetus, Igf2 i...   neutral\n",
       "14  Other studies suggest that altering affinity t...  positive\n",
       "15  Although our cohort also consisted of treatmen...  negative\n",
       "16  These study areas have all been managed in the...   neutral\n",
       "17  Assessing Risk of Bias\\nCritical appraisal of ...   neutral\n",
       "18  A retrospective cohort study abstracted medica...   neutral\n",
       "19  Cancer cells usually have a different genome s...   neutral\n",
       "20  %) was a close relative (at 97% identity) of t...   neutral\n",
       "21  Moreover, phytochelatins have been found to be...  positive\n",
       "22  One of the key concerns by induction of reprog...   neutral\n",
       "23  In [1] and [2], a multilayer directory structu...   neutral\n",
       "24  difference between aspirated and lenis is mer...   neutral"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert_pd_test = pd.DataFrame()\n",
    "bert_pd_test['text'], bert_pd_test['label'] = m_df_annotated['string'], m_df_annotated['sentiment_chris']\n",
    "bert_pd_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "b3fa00f4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train = datasets.Dataset.from_pandas(bert_pd)\n",
    "ds_train = ds_train.remove_columns('__index_level_0__')\n",
    "ds_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "085424b5",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'label'],\n",
       "    num_rows: 25\n",
       "})"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_test = datasets.Dataset.from_pandas(bert_pd_test)\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "0fee5010",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Utilizing gain-offunction assays, we demonstrate that enhancing actin polymer-\\\\nthese proteins does not increase SV density (Scheiffele et al., 2000; Bamji et al., 2003; Sara et al., 2005; Latefi et al., 2009; Linhoff et al., 2009).',\n",
       " 'maticity, refractoriness and conduction of the heart are conserved between zebrafish and humans [44, 46].',\n",
       " 'For reovirus 1/L-induced ARDS, at day 9 postinoculation, there is a severe pneumonia (peribronchiolar lesions with lymphocytic infiltration) with the presence of hyaline membranes, which are pathognomonic for human ARDS (20, 21).',\n",
       " 'Besides that, no ameliorate impacts have been obtained on the use of antioxidants in some studies (12,13).',\n",
       " 'However, limited information is available about molecular mechanisms of HM tolerance and detoxification in trees, and most of HM related genes in woody plants are restricted to Populus (Luo et al., 2016).',\n",
       " 'A recent qualitative study explored the views of individuals living with SCI towards the descriptive systems of six established instruments (15D, AQoL 8-dimension questionnaire (AQoL-8D) [28], EQ5D-5L [29], HUI, QWB-SA [30] and the SF-36v2 [31]) [32].',\n",
       " 'In\\\\naddition, there are continued new developments in lowcost, light-weight, and long-duration UAVs (Lucieer et al. 2014).',\n",
       " 'These results are consistent with our previous in vitro finding that Sef loss can enhance FGF-mediated induction of MMP9 in prostate cancer cells, which facilitates tumour cell invasion and migration [20].',\n",
       " 'Typically however, when a fungus was isolated from a seed, it was the only isolate, and roughly 74% seeds were free of all culturable fungi (Shipunov et al., 2008).',\n",
       " 'Subsequently, Shirure et al have shown that MAC-2BP carrying sLex was functionally involved in ZR-75-1 rolling on HUVECs cultured as monolayer (8).',\n",
       " 'It has been demonstrated in numerous experiments that anonymous pre-play communication increases cooperation in social dilemmas and improves bargaining efficiency (Sally, 1995; Camerer, 2003; Valley et al., 2002; Ellingsen and Johannesson, 2004).',\n",
       " 'Eicosanoids derived from AA, including PGE2, TBX2, and LTB4, are more proinflammatory than those derived from DHA (Calder, 2006). The results of our study confirmed this pathway of eicosanoid formation. Chickens fed LR diets had significantly lower levels of PGE2 and TBX2 in the blood than those fed HR diets. These results are in agreement with those of Cherian et al. (2009), who showed that the level of incorporation of n-3 FA was higher, but that of AA in cardiac ventricle lipids was lower, and the levels of PGE2 and TBX2 in the blood of progeny (7-, 14-, 28- and 42-day-old chickens) were lower after feeding HR diets to broiler hens. Liu et al. (2014) reported significantly lower levels of pro-inflammatory eicosanoids (PGE2 and TBX2) in the plasma due to inhibition of phospholipase A2 activity in chickens fed diets rich in PUFA n-3 than in chickens fed diets rich in PUFA n-6.',\n",
       " 'Up to almost 70 percent of patients with NS-associated FSGS may attain complete or partial remission and maintain stable renal function for about 10 years when given a prolonged therapy with steroid or immunosuppressive drugs (19).',\n",
       " 'As the ALSOVA study did not use a validated instrument to rate pain, somatic symptoms (symptoms of discomfort) were evaluated for their association with analgesic use, and assessed with one screening question discomfort and symptoms from the 15D self-administered, validated, health-related QoL questionnaire [31].',\n",
       " 'The computational camera with the coded apertures is able to estimate the depth of the scene in a single image [3].',\n",
       " 'Pricing via a coherent risk measure was first introduced by Artzner et al. (1999) and it reduces to a search for the infimum3 of all risk measures over an acceptance set.',\n",
       " 'further or more often to meet the energetic requirements of their members (Milton 1984; Janson 1988; Wrangham et al. 1993; Chapman et al. 1995), and previous studies on Amboseli groups have found that dry periods are associated with increased time spent foraging (Bronikowski and Altmann 1996).',\n",
       " 'The APP response has been observed in common pig infections (Grau-Roma et al., 2009; Pomorska-Mol et al., 2011a, b) but it is not known whether infection with H1N2 swine influenza virus (SIV) also induces an APP response.',\n",
       " 'lingual phrase-based machine translation system with a reranking post-processing step8 (Wubben et al., 2012) and Hybrid, a model which first performs sentence splitting and deletion operations over discourse representation structures and then further simplifies sentences with PBMT-R (Narayan and Gardent, 2014).',\n",
       " 'radiata are usually diurnal [30], [37] herbivores, whose broad diets occasionally include non-vegetative matter, such as carrion [38], [39].',\n",
       " 'H1-receptor binding studies: Inhibition of agonist-induced calcium flux was measured in CHO cells expressing the H1-receptor using the method described in [24].',\n",
       " 'During infection, C. neoformans cells of varying cell body size and capsule size are observed (Feldmesser et al., 2001; Okagaki et al., 2010; Zaragoza et al., 2010; Alanio et al., 2015).',\n",
       " 'To further characterize isolates, random amplification of polymorphic DNA polymerase chain reaction (RAPD-PCR) was performed using an adaptation of methods described elsewhere.(10)',\n",
       " 'It was also observed that CP treatment increased BUN and creatinine levels (Sadzuka et al., 1992; Mansour et al., 2002).',\n",
       " 'In caveolae membranes isolated from total plasma membranes using the methods of OptiPrep gradient centrifugation (35, 40) (Fig.',\n",
       " 'In particular, our model results are largely consistent with a classical description of the neurophonic as a dipole field generated by postsynaptic currents in MSO neurons [3,4].',\n",
       " 'Overall, we confirm the results observed by Ferrero et al. (2007a) on the effect of radical surgery for endometriosis in women with moderate to severe deep dyspareunia.',\n",
       " 'After being presented by Jacobs and Bruscos (1995), diverse IG-based algorithms have been applied to solve various scheduling problems, including single-machine scheduling problems (Ying et al, 2009), parallel-machine scheduling problems (Lin et al, 2011), flowshop scheduling problems (Ribas et al, 2011; Pan and Ruiz, 2014), HFSPs (Ying, 2009; Ying et al, 2014), distributed flowshop scheduling problems (Lin et al, 2013b; Fernandez-Viagas and Framinan, 2015; Ribas et al, 2017), and personnel task scheduling problems (Lin and Ying, 2014).',\n",
       " 'If medical evaluation reveals systemic lymphoma, treatment of the ocular disease is secondary to systemic treatment (usually chemotherapy).(1-30) The effectiveness of EBRT for the treatment of lymphoma of the eye and its adnexa has been widely documented.',\n",
       " 'Keddy 1989; see the review by Pausas and Austin 2001 and disturbance frequency gradients Connell 1978; Lubchenco 1978; Huston 1979; Wilson and Keddy 1988 could also be applied to the time since abandonment e.g., Peet 1978 , although the underlying factors may be different in each of these cases.',\n",
       " 'Although extensive research have been carried out on testing the ISSM [1619], little study [6] exists which explores the success of ePortfolios by leveraging ISSM as a theoretical underpinning.',\n",
       " 'We conclude that the FSClow OX1low cells represent microglia which are smaller than macrophages and lymphocytes (Milligan et al., 1991) and have a lower OX1 expression than haematogenously derived cells (Sedgwick et al., 1991).',\n",
       " 'The higher value for NST than GST would be indicative of a phylogeographic structure (Pons and Petit 1996  Petit et al. 2005).',\n",
       " 'The literature (Sugihara and Smith, 1997; Shahidi et al., 2004; Gerke, 1999; Gao and Tian, 2007; Zhao and Yan, 2005) shows most of the non-traditional algorithms are applied only for regular convex polygonal obstacles, not for irregular convex, concave polygonal or curved obstacles.',\n",
       " 'After the slow phase of cellularization, the network shifts into a fast phase of rapid furrow ingression and membrane addition is targeted to an apicolateral location (Lecuit and Wieschaus, 2000).',\n",
       " 'The D-HOTM system is based on the Text Mining Infrast ucture (TMI) developed by the parallel and distributed text mining lab at Lehigh University [20].',\n",
       " 'These results are supported by previous published data highlighting that the miR-34 family, including miR-34a and miR-34b/c, acts as a tumor suppressor, able to inhibit different human tumor growth and invasion [3639], including human LSCC [3134].',\n",
       " 'This effect is virtually identical to what was reported for cells treated with cyclodextrin (Pontier et al., 2008; Allen et al., 2009).',\n",
       " 'bacterial species can drive major changes in the gut microbiota and, thus, influence the metabolic profile (Marcobal et al., 2013).',\n",
       " 'We have sampled their data with 4 different methods: the insertion sorting algorithm (IS) and 2 variations of it and the binary tree method [7, 3].',\n",
       " 'Cell viability results showing that lung cancer cells exhibit resistance to the cytotoxicity of high concentration of ATP in comparison to normal cells are consistent with published data that showed low (nearly undetectable) expression levels of P2X7R in lung cancer cells (20, 21, 39).',\n",
       " 'The absolute pose estimation of a perspective camera from n 2D3D point correspondences is known in the literature as the Perspective n Point (PnP) problem, which has been widely studied in the last few decades [6, 13, 14, 8].',\n",
       " 'Taken from the revised Chinese version of the Health Literacy Management Scale, this questionnaire was designed to cover multiple items related to health literacy, which are consistent with the theoretical content of health literacy as noted in the previous literature (1,2).',\n",
       " 'This conclusion is consistent with previous results (7, 21, 22) that suggest that the increase in ALC produced by the b-adrenergic agonists isoproterenol and terbutaline is characterized by an increase in the proportion of the total sodium transport that is amiloride-sensitive.',\n",
       " 'Over the last few years, many different local interest point detectors (like Harris3D [16], Cuboid [6], Hessian [35] or Dense sampling [33]) and many spatio-temporal descriptors (like HOG [17], HOG3D [14], HOF [17], Cuboid [6] or ESURF [35]) have been proposed.',\n",
       " 'While beaked whales and large delphinids produce clicks with comparable energy flux source levels (Madsen et al. 2004, Zimmer et al. 2005, Au et al. 2007), they may hunt in quite different ambient noise environments.',\n",
       " 'Rutin (3,3,4,5,7-pentahydroxyflavone-3-rhamnoglucoside, Figure 2) also called sophorin, rutoside, and quercetin-3rutinoside is a polyphenolic bioflavonoid, largely extracted from natural sources such as oranges, lemons, grapes, limes, berries, and peaches [30, 31].',\n",
       " 'Interest in the use of CWs for remediation of runoff from agricultural irrigation and agro-industrial production has become increasingly popular over the last decades, due to their low capital and operational cost, low energy consumption, and environmental friendliness (Huett et al., 2005; Scholz, 2007; Carty et al., 2008).',\n",
       " 'PAR has been described using the image of a spiral (Williamson et al., 2004), and various authors report the suitability of the application of this method to the management of change within organizations (Coghlan and Casey, 2001; Coyer et al., 2007; Kelly and Simpson 2001).',\n",
       " 'These effects of locomotion on the baseline activity of Sst interneurons confirm some previous results (Polack et al., 2013) but they appear to disagree with other measurements (Fu et al., 2014).',\n",
       " 'The relationship between DNA content and absorbance at 600 nm is linear [17], allowing the amount of DNA in each unknown sample to be determined from the standard curve.',\n",
       " 'With respect to the contradicting results of other studies (Licausi et al., 2011; Weits et al., 2014), we propose that the air humidity during the recovery\\\\nperiod is a crucial factor for the length of survival of submergence in darkness, which was shown here for treatment of the different genotypes',\n",
       " 'We elected to take this course because formal methods for analyzing the efficiency of move ordering strategies in the alpha-beta search are unreported in the literature [27, 30], and as a result, practical experimentation is the accepted form of analysis in the state-of-the-art [26].',\n",
       " 'Nuclear localization of PTOV1 is required for the stimulation of cell proliferation (Santamaria et al. 2005; Santamaria et al. 2003).',\n",
       " 'The remaining medial portion of the bone was potted (Codsi and Iannotti, 2008, Hoenig et al., 2009, Kwon et al., 2010) in an epoxy resin cylinder (Struers A/S, Ballerup, Denmark) to obtain a secure fixation of the bone to the set up while simultaneously isolating the bone from deformations induced',\n",
       " 'The occurrence of soft tissue tumors in such field of low-dose irradiation (LDRI) has also been emphasized by other authors [50, 52, 53] reporting the development of tumors of the skull bone and neck after exposure to full-mouth diagnostic dental X-ray film [3] and to irradiation for enlarged tonsils [54] and thymus [55].',\n",
       " 'We consider 100 million single (and early) simulation points as determined by SimPoint [24], [25] in all of our experiments.',\n",
       " 'Third, we used a meta-analysis technique to assemble the results of eligible studies, and the pooled results were presented in the format of quantitative data; whereas in the systematic review by Jiang et al, they only described the previous relevant studies and did not give a quantitative analysis for these studies.(13) Although studies comparing between ACDF and ACCF have been done, the optimal approach that provides better clinical effects in the treatment of CSM remains controversial.',\n",
       " 'Obesity is the central and causal component in metabolic syndrome and adipocytes produce a variety of biologically active molecules [9].',\n",
       " 'A private key is required in the verifier side to generate N pseudo random locations to be used for retrieving the watermark; (2) a watermark is embedded into two or more halftoning images, where the retrieval approaches can be overlays of halftone images [15] or using a look up table (LUT) to decode the watermark [12].',\n",
       " 'Following precipitation with 0.3MNaAc and 2 volume of ethanol, the RNA was treated with DNAseI (Sigma) and quantified in a NanoDrop 1000 Spectrophotometer.',\n",
       " 'Mammalian Rap1 isoforms are perhaps best known for regulating integrinbased cell matrix adhesion (Bos, 2005; Kim et al., 2011), but Rap1 also regulates cellcell AJs in both Drosophila and mice (Kooistra et al., 2007; Boettner and Van Aelst, 2009).',\n",
       " 'The Intercontinental Chemical Transport Experiment B (INTEX-B) aircraft campaign over the North Pacific and western coast of North America made measurements of OH\\\\nand HO2 (Mao et al., 2009; Singh et al., 2009).',\n",
       " 'ICP-MS reports of sub-parts per billion detection limits, multielement analysis capabilities, and a wide linear dynamic range have made it an often used technique for wine elemental analysis [9].',\n",
       " 'In fact, Underwood (1957) made this very point when he observed that the sleeping subjects in Jenkins and Dallenbachs (1924) classic sleep study did not avoid learning material that was similar to the study list, so avoidance of retroactive interference could not explain why sleep facilitates',\n",
       " 'EPA is also shown to attenuate renal fibrosis and TGF- 1 expression in a diabetic mice model (44).',\n",
       " '8\\\\nTo validate the quality of our categorization, the next section discusses series of experiments that compare the performance of the vector space categorization approach with one of the top classifiers, Nave-Bayes (Friedman et al., 1997).',\n",
       " 'These were derived from prior non-operative studies for inguinal hernia and ventral hernia [15, 23, 28, 29].',\n",
       " 'Gouveia and Martins [7] reviewed exact and lower bounding schemes, including earlier works of Gavish [4, 5], the branch-and-bound algorithm of Malik and Yu [10], the Lagrangean relaxation approach of Gouveia [6], and the cutting plane method of Hall [9].',\n",
       " '2010)\\\\nLack of HPWS/HRM in Victorian health care organisations\\\\nVictorian state public health care organisations report limited HRM/HPWS components (Leggat et al., 2008) Victorian state public hospitals report fewer aspects of HPWS then community health services (Leggat et al., 2006) CEOs, HR',\n",
       " 'Neurotrophins have been proposed to play a role in neuronal survival, proliferation, maturation, and outgrowth during development, and to show neuroprotective functions after brain injury (3, 4).',\n",
       " 'Gadolinium enhancement of the inflamed tissues is usually present with MRI.(15) The present case varied slightly in that the foreign body was isointense to hyperintense on precontrast T1-weighted images with the surrounding inflammatory reaction also being hyperintense to muscle.',\n",
       " 'Northern blotting\\\\n The ORF sequence for HvNAS1 (AB010086) was used to create an HvNAS1 probe (Suzuki et al. 2006).',\n",
       " 'Findings of difficulties in locating the presented tactile stimuli, which were space- and not arm-based [32], and difficulties in deciding on the exact position and orientation of the limb or on the laterality of pictured hands in limb laterality recognition tests [29,42,48], are reminiscent of patients with poststroke',\n",
       " 'An interdisciplinary approach has been advocated for in a broad range of healthcare fields, including primary care [1, 2], developmental disability assessment [3], com-',\n",
       " 'This i Attribution License (http://creativecommons.or in any medium, provided the original work is p\\\\nthe formation and evolution of meteorite parent bodies (Ebihara et al. 2011; Nagao et al. 2011; Nakamura et al. 2011; Noguchi et al. 2011; Tsuchiyama et al. 2011; Yurimoto et al. 2011).',\n",
       " 'represent phases in a progressive deformation, with layer thickness as the evolving parameter [Ghent and Hansen, 1999].',\n",
       " '[30] focus on returning the fragments rooted at SLCA nodes, and propose an XML keyword search engine XSeek.',\n",
       " 'The ethanol was gradually introduced into the diet over an 8-day period, as described previously (22, 24).',\n",
       " 'com) was used to digitally capture the referent and simulation data from [17].',\n",
       " 'Frequent methylation for TIMP3 (Kang et al. 2006) and CDH13 (Suehiro et al. 2008) in the present study concurs with previously published results in endometrial cancer.',\n",
       " 'This dysbiosis may impact the presence of interleukin-17 (IL17)-producing T-helper cells (Th17) and regulatory T cells (Tregs), as the development of these cells is heavily reliant on the microbial composition (Atarashi et al. 2011; Ivanov et al. 2009; Kriegel et al. 2011).',\n",
       " 'ported lower levels in cases (15, 16, 19, 22); four reported higher levels in cases than in controls (18, 20, 23, 27); and one reported equal levels in cases and controls (17).',\n",
       " 'delayed in monoamine oxidase A-deficient (Tg8) neonatal mice that display high 5-HT levels in the CNS (Cazalets et al., 2000).',\n",
       " 'Concerning the adaptation to HMs in plants, woody plants and herbaceous may possess similar physiological and molecular mechanisms underlying HM uptake, transport, sequestration, and detoxification (Luo et al., 2016).',\n",
       " 'The importance of reliable M-shell Fe DR data has also been shown by the analyses of recent Chandra and XMM AGN spectra.',\n",
       " 'Several sample preparation methods have been employed for the elemental analysis of wines, including direct dilution by factors ranging from one to 10, UV-assisted digestion, and open or closed vessel acid digestion assisted by microwave radiation [4,612].',\n",
       " 'Although it forms the largest ethnic minority group in the UK, the Asian community is not a culturally homogeneous group, a fact that has not been addressed in previous studies of childhood cancer in Asians (Stiller et al., 1991; Powell et al., 1994).',\n",
       " 'The epidemic curves in the Montreal network occasionally exhibit multiple peaks, driven by underlying community structure [56].',\n",
       " 'Furthermore, tumor-associated macrophages secrete products, such as TGF- and IL-10 that suppress anti-tumor immune responses [14, 20, 10].',\n",
       " 'This model has been improved by means of techniques and instruments of data acquisition: particularly, through the development of statistical packages applied to the different factors as well as through GIS softwares, used for the management and processing of georeferenced data for large areas (Johnston, 1998; Meijerink et al., 2003; Daniels, 2003).',\n",
       " 'Differences between central (aortic) and brachial BP could be clinically important, as central BP is more closely correlated with target organ damage in hypertension [4]; is differentially influenced by antihypertensive therapy compared with brachial BP [5]; and may be a superior predictor of cardiovascular events than brachial BP [6].',\n",
       " 'In this section, we first present experimental results on CIFAR-10 [20] and ImageNet dataset [5] to justify that pattern binarization could reduce the effective number of parameters dramatically and fine-tuning other parameters of the binarized network with fixed binarized pattern could achieve comparable performance to that of the original neural network models.',\n",
       " 'Approximately 1/3 of the glucan in DGS is starch, indicating the need for amylase as well as cellulase (Kim et al., 2008a).',\n",
       " 'Integrate the spectral and spatial information with the time component provides rich information to detail the space variations along the time [Petitjean et al., 2012] and can provide pattern observations, which are not found in single time observations, such as trends and periodicities [Kuenzer',\n",
       " 'A recent review by Kazan et al depicted current advancements in FHB pathogenomics and host resistance [26].',\n",
       " 'is variation among ecosystems, the most common eVect of exposing plants to UV-B is a decrease in the intensity of insect herbivory (Ballar et al. 1996; Rousseaux et al. 1998, 2004; Mazza et al. 1999b; Zavala et al. 2001; for reviews, see Ballar et al. 2001; Caldwell et al. 2003; Bassman 2004).',\n",
       " 'Thirdly, ErbB-3 is characterized by a defective mechanism of clearance from the cell surface (Baulida et al., 1996; Pinkas-Kramarski et al., 1996): instead of ligand-induced internalization and degradation in lysosomes, a mechanism shared by many growth factor receptors (Sorkin and Waters, 1993), ErbB-3 is destined primarily to enter the recycling pathway (Waterman et al.',\n",
       " 'These clusters of informative voxels, along with a cluster implicating the occipital cortex, have also been consistently found in neuroimaging studies using statistical parametric mapping (Bush et al., 2000; Colibazzi et al., 2010; Gerber et al., 2008; Hagan et al., 2009; Killgore and Yurgelun-Todd, 2004; Narumoto et al., 2001; Posner et al., 2009; Robins et al., 2009; Wager et al., 2008).',\n",
       " 'Combined arterial and venous thromboembolism was observed in six [14, 19, 25, 36].']"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_train['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfa809b4",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1091cb1ca63a4afc9caf5050411d8061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bffda0ad5bdd4cc7be61b6fc8fb7ab78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def preprocess_function(examples):\n",
    "   return tokenizer(examples[\"text\"], truncation=True, padding=True)\n",
    "\n",
    "\"\"\"tokenized_train = ds_train.map(preprocess_function, batched=True,\n",
    "                               #remove_columns=ds_train.column_names\n",
    "                               )\n",
    "tokenized_test = ds_test.map(preprocess_function, batched=True,\n",
    "                             #remove_columns=ds_test.column_names\n",
    "                             )\"\"\"\n",
    "tokenized_train = small_train_dataset.map(preprocess_function, batched=True)\n",
    "tokenized_test = small_test_dataset.map(preprocess_function, batched=True)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "54858738",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['attention_mask', 'input_ids', 'label', 'text'],\n",
       "    num_rows: 3000\n",
       "})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a113ee2f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.weight', 'vocab_projector.weight', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e40f84f1",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric, ClassLabel\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "   load_accuracy = load_metric(\"accuracy\")\n",
    "   load_f1 = load_metric(\"f1\")\n",
    "\n",
    "   logits, labels = eval_pred\n",
    "   predictions = np.argmax(logits, axis=-1)\n",
    "   accuracy = load_accuracy.compute(predictions=predictions, references=labels)[\"accuracy\"]\n",
    "   f1 = load_f1.compute(predictions=predictions, references=labels)[\"f1\"]\n",
    "   return {\"accuracy\": accuracy, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5fd164cc",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "   output_dir=\"bert_test\",\n",
    "   learning_rate=2e-5,\n",
    "   per_device_train_batch_size=8,\n",
    "   per_device_eval_batch_size=8,\n",
    "   num_train_epochs=5,\n",
    "   weight_decay=0.01,\n",
    "   save_strategy=\"epoch\",\n",
    "   gradient_accumulation_steps=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fff2984e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "   model=model,\n",
    "   args=training_args,\n",
    "   train_dataset=tokenized_train,\n",
    "   eval_dataset=tokenized_test,\n",
    "   tokenizer=tokenizer,\n",
    "   data_collator=data_collator,\n",
    "   compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e523fb39",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `DistilBertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `DistilBertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 3000\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 2\n",
      "  Total optimization steps = 935\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.76 GiB total capacity; 5.50 GiB already allocated; 19.75 MiB free; 5.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Input \u001B[0;32mIn [13]\u001B[0m, in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrain\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/trainer.py:1375\u001B[0m, in \u001B[0;36mTrainer.train\u001B[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001B[0m\n\u001B[1;32m   1373\u001B[0m         tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtraining_step(model, inputs)\n\u001B[1;32m   1374\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 1375\u001B[0m     tr_loss_step \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtraining_step\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1377\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[1;32m   1378\u001B[0m     args\u001B[38;5;241m.\u001B[39mlogging_nan_inf_filter\n\u001B[1;32m   1379\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m is_torch_tpu_available()\n\u001B[1;32m   1380\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m (torch\u001B[38;5;241m.\u001B[39misnan(tr_loss_step) \u001B[38;5;129;01mor\u001B[39;00m torch\u001B[38;5;241m.\u001B[39misinf(tr_loss_step))\n\u001B[1;32m   1381\u001B[0m ):\n\u001B[1;32m   1382\u001B[0m     \u001B[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001B[39;00m\n\u001B[1;32m   1383\u001B[0m     tr_loss \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m tr_loss \u001B[38;5;241m/\u001B[39m (\u001B[38;5;241m1\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mglobal_step \u001B[38;5;241m-\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_globalstep_last_logged)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/trainer.py:1959\u001B[0m, in \u001B[0;36mTrainer.training_step\u001B[0;34m(self, model, inputs)\u001B[0m\n\u001B[1;32m   1956\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m loss_mb\u001B[38;5;241m.\u001B[39mreduce_mean()\u001B[38;5;241m.\u001B[39mdetach()\u001B[38;5;241m.\u001B[39mto(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mdevice)\n\u001B[1;32m   1958\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mautocast_smart_context_manager():\n\u001B[0;32m-> 1959\u001B[0m     loss \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_loss\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1961\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mn_gpu \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m   1962\u001B[0m     loss \u001B[38;5;241m=\u001B[39m loss\u001B[38;5;241m.\u001B[39mmean()  \u001B[38;5;66;03m# mean() to average on multi-gpu parallel training\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/trainer.py:1991\u001B[0m, in \u001B[0;36mTrainer.compute_loss\u001B[0;34m(self, model, inputs, return_outputs)\u001B[0m\n\u001B[1;32m   1989\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m   1990\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[0;32m-> 1991\u001B[0m outputs \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1992\u001B[0m \u001B[38;5;66;03m# Save past state if it exists\u001B[39;00m\n\u001B[1;32m   1993\u001B[0m \u001B[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001B[39;00m\n\u001B[1;32m   1994\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mpast_index \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:744\u001B[0m, in \u001B[0;36mDistilBertForSequenceClassification.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    736\u001B[0m \u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    737\u001B[0m \u001B[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001B[39;00m\n\u001B[1;32m    738\u001B[0m \u001B[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001B[39;00m\n\u001B[1;32m    739\u001B[0m \u001B[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001B[39;00m\n\u001B[1;32m    740\u001B[0m \u001B[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001B[39;00m\n\u001B[1;32m    741\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    742\u001B[0m return_dict \u001B[38;5;241m=\u001B[39m return_dict \u001B[38;5;28;01mif\u001B[39;00m return_dict \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_return_dict\n\u001B[0;32m--> 744\u001B[0m distilbert_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdistilbert\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    745\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_ids\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_ids\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    746\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattention_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    747\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    748\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs_embeds\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    749\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    750\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    751\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    752\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    753\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m distilbert_output[\u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, seq_len, dim)\u001B[39;00m\n\u001B[1;32m    754\u001B[0m pooled_output \u001B[38;5;241m=\u001B[39m hidden_state[:, \u001B[38;5;241m0\u001B[39m]  \u001B[38;5;66;03m# (bs, dim)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/adapters/context.py:103\u001B[0m, in \u001B[0;36mForwardContext.wrap.<locals>.wrapper_func\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    101\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39madapters \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    102\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mcls\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[0;32m--> 103\u001B[0m         results \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    104\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m results\n\u001B[1;32m    105\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:565\u001B[0m, in \u001B[0;36mDistilBertModel.forward\u001B[0;34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    562\u001B[0m     inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39membeddings(input_ids)  \u001B[38;5;66;03m# (bs, seq_length, dim)\u001B[39;00m\n\u001B[1;32m    563\u001B[0m inputs_embeds \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minvertible_adapters_forward(inputs_embeds)\n\u001B[0;32m--> 565\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransformer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    566\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minputs_embeds\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    567\u001B[0m \u001B[43m    \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattention_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    568\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    569\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    570\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_hidden_states\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_hidden_states\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    571\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_dict\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_dict\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    572\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:338\u001B[0m, in \u001B[0;36mTransformer.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001B[0m\n\u001B[1;32m    335\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_hidden_states:\n\u001B[1;32m    336\u001B[0m     all_hidden_states \u001B[38;5;241m=\u001B[39m all_hidden_states \u001B[38;5;241m+\u001B[39m (hidden_state,)\n\u001B[0;32m--> 338\u001B[0m layer_outputs \u001B[38;5;241m=\u001B[39m \u001B[43mlayer_module\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    339\u001B[0m \u001B[43m    \u001B[49m\u001B[43mx\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhidden_state\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mattn_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\n\u001B[1;32m    340\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    341\u001B[0m hidden_state \u001B[38;5;241m=\u001B[39m layer_outputs[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m]\n\u001B[1;32m    342\u001B[0m (attn_mask,) \u001B[38;5;241m=\u001B[39m adjust_tensors_for_parallel(hidden_state, attn_mask)\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:282\u001B[0m, in \u001B[0;36mTransformerBlock.forward\u001B[0;34m(self, x, attn_mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    272\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    273\u001B[0m \u001B[38;5;124;03mParameters:\u001B[39;00m\n\u001B[1;32m    274\u001B[0m \u001B[38;5;124;03m    x: torch.tensor(bs, seq_length, dim)\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    279\u001B[0m \u001B[38;5;124;03m    torch.tensor(bs, seq_length, dim) The output of the transformer block contextualization.\u001B[39;00m\n\u001B[1;32m    280\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    281\u001B[0m \u001B[38;5;66;03m# Self-Attention\u001B[39;00m\n\u001B[0;32m--> 282\u001B[0m sa_output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mattention\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    283\u001B[0m \u001B[43m    \u001B[49m\u001B[43mquery\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    284\u001B[0m \u001B[43m    \u001B[49m\u001B[43mkey\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    285\u001B[0m \u001B[43m    \u001B[49m\u001B[43mvalue\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    286\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mattn_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    287\u001B[0m \u001B[43m    \u001B[49m\u001B[43mhead_mask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mhead_mask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    288\u001B[0m \u001B[43m    \u001B[49m\u001B[43moutput_attentions\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moutput_attentions\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    289\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    290\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m output_attentions:\n\u001B[1;32m    291\u001B[0m     sa_output, sa_weights \u001B[38;5;241m=\u001B[39m sa_output  \u001B[38;5;66;03m# (bs, seq_length, dim), (bs, n_heads, seq_length, seq_length)\u001B[39;00m\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/torch/nn/modules/module.py:1131\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[0;34m(self, *input, **kwargs)\u001B[0m\n\u001B[1;32m   1127\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[1;32m   1128\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[1;32m   1129\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[1;32m   1130\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[0;32m-> 1131\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1132\u001B[0m \u001B[38;5;66;03m# Do not call functions when jit is used\u001B[39;00m\n\u001B[1;32m   1133\u001B[0m full_backward_hooks, non_full_backward_hooks \u001B[38;5;241m=\u001B[39m [], []\n",
      "File \u001B[0;32m~/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/models/distilbert/modeling_distilbert.py:215\u001B[0m, in \u001B[0;36mMultiHeadSelfAttention.forward\u001B[0;34m(self, query, key, value, mask, head_mask, output_attentions)\u001B[0m\n\u001B[1;32m    213\u001B[0m scores \u001B[38;5;241m=\u001B[39m torch\u001B[38;5;241m.\u001B[39mmatmul(q, k\u001B[38;5;241m.\u001B[39mtranspose(\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m3\u001B[39m))  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    214\u001B[0m mask \u001B[38;5;241m=\u001B[39m (mask \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\u001B[38;5;241m.\u001B[39mview(mask_reshp)\u001B[38;5;241m.\u001B[39mexpand_as(scores)  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[0;32m--> 215\u001B[0m scores \u001B[38;5;241m=\u001B[39m \u001B[43mscores\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmasked_fill\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmask\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;28;43mfloat\u001B[39;49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minf\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    217\u001B[0m weights \u001B[38;5;241m=\u001B[39m nn\u001B[38;5;241m.\u001B[39mfunctional\u001B[38;5;241m.\u001B[39msoftmax(scores, dim\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n\u001B[1;32m    218\u001B[0m weights \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdropout(weights)  \u001B[38;5;66;03m# (bs, n_heads, q_length, k_length)\u001B[39;00m\n",
      "\u001B[0;31mRuntimeError\u001B[0m: CUDA out of memory. Tried to allocate 96.00 MiB (GPU 0; 7.76 GiB total capacity; 5.50 GiB already allocated; 19.75 MiB free; 5.54 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Optuna Hyper Search"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "import datasets\n",
    "import optuna\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "from datasets import load_metric, ClassLabel\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-947c940d5fb73b0e\n",
      "Reusing dataset csv (/home/lolai/.cache/huggingface/datasets/csv/default-947c940d5fb73b0e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a524a6a2cf9541f1ae60005bc4dcc1bf"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Make the kfold object\n",
    "folds = StratifiedKFold(n_splits=5)\n",
    "# Then get the dataset\n",
    "data = datasets.load_dataset('csv', data_files={'train':'sentimentAnnotations_CSV/annotation_450.csv'}).shuffle()\n",
    "\n",
    "# Splits based off labels.\n",
    "splits = folds.split(np.zeros(data[\"train\"].num_rows), data[\"train\"][\"label\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-947c940d5fb73b0e\n",
      "Reusing dataset csv (/home/lolai/.cache/huggingface/datasets/csv/default-947c940d5fb73b0e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c0dd14264184bcb8cb2b0406f8c6c97"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/vocab.txt from cache at /home/lolai/.cache/huggingface/transformers/0e1bbfda7f63a99bb52e3915dcf10c3c92122b827d92eb2d34ce94ee79ba486c.d789d64ebfe299b0e416afc4a169632f903f693095b4629a7ea271d5a0cf2c99\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer.json from cache at /home/lolai/.cache/huggingface/transformers/75abb59d7a06f4f640158a9bfcde005264e59e8d566781ab1415b139d2e4c603.7f2721073f19841be16f41b0a70b600ca6b880c8f3df6f3535cbc704371bdfa4\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/distilbert-base-uncased/resolve/main/tokenizer_config.json from cache at /home/lolai/.cache/huggingface/transformers/8c8624b8ac8aa99c60c912161f8332de003484428c47906d7ff7eb7f73eecdbb.20430bd8e10ef77a7d2977accefe796051e01bc2fc4aa146bc862997a1a15e79\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "Loading cached processed dataset at /home/lolai/.cache/huggingface/datasets/csv/default-947c940d5fb73b0e/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e/cache-619ca7ea229add5e.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f1c47064a3af47a98dbe5457148ae956"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "metric = load_metric(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def compute_metrics2(p):\n",
    "    pred, labels = p\n",
    "    pred = np.argmax(pred, axis=1)\n",
    "\n",
    "    accuracy = accuracy_score(y_true=labels, y_pred=pred)\n",
    "    recall = recall_score(y_true=labels, y_pred=pred, average=\"macro\")\n",
    "    precision = precision_score(y_true=labels, y_pred=pred, average=\"macro\")\n",
    "    f1 = f1_score(y_true=labels, y_pred=pred, average=\"macro\")\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}\n",
    "\n",
    "scores = {}\n",
    "# Override train/test\n",
    "for split in splits:\n",
    "    splitx = split\n",
    "data = None\n",
    "data = datasets.load_dataset('csv', data_files={'train':'sentimentAnnotations_CSV/annotation_450.csv'})\n",
    "data['test'] = data['train'].select(splitx[1])\n",
    "data['train'] = data['train'].select(splitx[0])\n",
    "labels = ClassLabel(num_classes=3, names=['positive', 'negative', 'neutral'])\n",
    "\n",
    "def preprocess_function(batch):\n",
    "    tokens = tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "    tokens['label'] = labels.str2int(batch['label'])\n",
    "    return tokens\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "tokenized_data = data.map(preprocess_function, batched=True,\n",
    "                          remove_columns=data[\"train\"].column_names\n",
    "                          )\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "def objective(trial: optuna.Trial):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=3)\n",
    "    training_args = TrainingArguments(output_dir='ade-test',\n",
    "                                      learning_rate=trial.suggest_loguniform('learning_rate', low=4e-5, high=0.01),\n",
    "                                      weight_decay=trial.suggest_loguniform('weight_decay', 4e-5, 0.01),\n",
    "                                      num_train_epochs=trial.suggest_int('num_train_epochs', low = 2,high = 5),\n",
    "                                    per_device_train_batch_size=8,\n",
    "                                    per_device_eval_batch_size=8,\n",
    "                                    disable_tqdm=True)\n",
    "    trainer = Trainer(model=model,\n",
    "                        args=training_args,\n",
    "                        train_dataset=tokenized_data['train'],\n",
    "                        eval_dataset=tokenized_data['test'],\n",
    "                        tokenizer=tokenizer,\n",
    "                        data_collator=data_collator,\n",
    "                        )\n",
    "    result = trainer.train()\n",
    "    return result.training_loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[I 2022-07-09 00:26:51,939]\u001B[0m A new study created in memory with name: hyper-parameter-search\u001B[0m\n",
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 235\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:27:34,657]\u001B[0m Trial 0 finished with value: 0.9095260295462101 and parameters: {'learning_rate': 0.0008472893955130622, 'weight_decay': 0.004314579689567447, 'num_train_epochs': 5}. Best is trial 0 with value: 0.9095260295462101.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 41.4512, 'train_samples_per_second': 45.355, 'train_steps_per_second': 5.669, 'train_loss': 0.9095260295462101, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:28:00,773]\u001B[0m Trial 1 finished with value: 0.904352661565686 and parameters: {'learning_rate': 0.0009716657303996851, 'weight_decay': 4.4348804809031176e-05, 'num_train_epochs': 3}. Best is trial 1 with value: 0.904352661565686.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.8442, 'train_samples_per_second': 45.403, 'train_steps_per_second': 5.675, 'train_loss': 0.904352661565686, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:28:34,538]\u001B[0m Trial 2 finished with value: 0.8182286201639378 and parameters: {'learning_rate': 0.0005114981527488084, 'weight_decay': 0.0006411555657476299, 'num_train_epochs': 4}. Best is trial 2 with value: 0.8182286201639378.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 32.5093, 'train_samples_per_second': 46.264, 'train_steps_per_second': 5.783, 'train_loss': 0.8182286201639378, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 94\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:28:51,429]\u001B[0m Trial 3 finished with value: 0.8116566272492104 and parameters: {'learning_rate': 0.00017573561390740722, 'weight_decay': 0.0003782113989471803, 'num_train_epochs': 2}. Best is trial 3 with value: 0.8116566272492104.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15.5855, 'train_samples_per_second': 48.25, 'train_steps_per_second': 6.031, 'train_loss': 0.8116566272492104, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:29:16,993]\u001B[0m Trial 4 finished with value: 0.8943296391913231 and parameters: {'learning_rate': 0.0003511593881758136, 'weight_decay': 0.0027168180668686024, 'num_train_epochs': 3}. Best is trial 3 with value: 0.8116566272492104.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.2444, 'train_samples_per_second': 46.526, 'train_steps_per_second': 5.816, 'train_loss': 0.8943296391913231, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:29:49,210]\u001B[0m Trial 5 finished with value: 0.90737687780502 and parameters: {'learning_rate': 0.0006739676167158297, 'weight_decay': 0.0009135771295594347, 'num_train_epochs': 4}. Best is trial 3 with value: 0.8116566272492104.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 30.8911, 'train_samples_per_second': 48.687, 'train_steps_per_second': 6.086, 'train_loss': 0.90737687780502, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 235\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:30:29,986]\u001B[0m Trial 6 finished with value: 0.8909566676363032 and parameters: {'learning_rate': 0.00031055982994867274, 'weight_decay': 0.001491792865567962, 'num_train_epochs': 5}. Best is trial 3 with value: 0.8116566272492104.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 39.4415, 'train_samples_per_second': 47.666, 'train_steps_per_second': 5.958, 'train_loss': 0.8909566676363032, 'epoch': 5.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:30:55,107]\u001B[0m Trial 7 finished with value: 0.6193275181114251 and parameters: {'learning_rate': 4.208084556036332e-05, 'weight_decay': 0.0011200615829707748, 'num_train_epochs': 3}. Best is trial 7 with value: 0.6193275181114251.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.7844, 'train_samples_per_second': 47.426, 'train_steps_per_second': 5.928, 'train_loss': 0.6193275181114251, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:31:21,121]\u001B[0m Trial 8 finished with value: 0.5931765508989916 and parameters: {'learning_rate': 5.4469448706747806e-05, 'weight_decay': 0.0001564059736264875, 'num_train_epochs': 3}. Best is trial 8 with value: 0.5931765508989916.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 24.6678, 'train_samples_per_second': 45.728, 'train_steps_per_second': 5.716, 'train_loss': 0.5931765508989916, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:31:45,715]\u001B[0m Trial 9 finished with value: 0.5813875671819593 and parameters: {'learning_rate': 5.785871177540348e-05, 'weight_decay': 0.0048504834121640686, 'num_train_epochs': 3}. Best is trial 9 with value: 0.5813875671819593.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.2888, 'train_samples_per_second': 48.435, 'train_steps_per_second': 6.054, 'train_loss': 0.5813875671819593, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 94\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:32:03,142]\u001B[0m Trial 10 finished with value: 1.4581470895320812 and parameters: {'learning_rate': 0.006048102611333968, 'weight_decay': 0.008814051558997008, 'num_train_epochs': 2}. Best is trial 9 with value: 0.5813875671819593.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 16.1414, 'train_samples_per_second': 46.588, 'train_steps_per_second': 5.824, 'train_loss': 1.4581470895320812, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 141\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:32:27,679]\u001B[0m Trial 11 finished with value: 0.5749863969518784 and parameters: {'learning_rate': 4.527594343013739e-05, 'weight_decay': 0.000167314907326541, 'num_train_epochs': 3}. Best is trial 11 with value: 0.5749863969518784.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 23.2503, 'train_samples_per_second': 48.516, 'train_steps_per_second': 6.064, 'train_loss': 0.5749863969518784, 'epoch': 3.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 94\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:32:44,395]\u001B[0m Trial 12 finished with value: 0.689926472116024 and parameters: {'learning_rate': 9.07614609602052e-05, 'weight_decay': 0.00017522435952955706, 'num_train_epochs': 2}. Best is trial 11 with value: 0.5749863969518784.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 15.438, 'train_samples_per_second': 48.711, 'train_steps_per_second': 6.089, 'train_loss': 0.689926472116024, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:33:16,562]\u001B[0m Trial 13 finished with value: 0.4329967904598155 and parameters: {'learning_rate': 0.00011715503310767902, 'weight_decay': 5.399698478570332e-05, 'num_train_epochs': 4}. Best is trial 13 with value: 0.4329967904598155.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 30.8798, 'train_samples_per_second': 48.705, 'train_steps_per_second': 6.088, 'train_loss': 0.4329967904598155, 'epoch': 4.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/distilbert-base-uncased/resolve/main/config.json from cache at /home/lolai/.cache/huggingface/transformers/23454919702d26495337f3da04d1655c7ee010d5ec9d77bdb9e399e00302c0a1.91b885ab15d631bf9cee9dc9d25ece0afd932f2f5130eba28f2055b2220c0333\n",
      "Model config DistilBertConfig {\n",
      "  \"_name_or_path\": \"distilbert-base-uncased\",\n",
      "  \"activation\": \"gelu\",\n",
      "  \"architectures\": [\n",
      "    \"DistilBertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.1,\n",
      "  \"dim\": 768,\n",
      "  \"dropout\": 0.1,\n",
      "  \"hidden_dim\": 3072,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"distilbert\",\n",
      "  \"n_heads\": 12,\n",
      "  \"n_layers\": 6,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"qa_dropout\": 0.1,\n",
      "  \"seq_classif_dropout\": 0.2,\n",
      "  \"sinusoidal_pos_embds\": false,\n",
      "  \"tie_weights_\": true,\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"vocab_size\": 30522\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/distilbert-base-uncased/resolve/main/pytorch_model.bin from cache at /home/lolai/.cache/huggingface/transformers/9c169103d7e5a73936dd2b627e42851bec0831212b677c637033ee4bce9ab5ee.126183e36667471617ae2f0835fab707baa54b731f991507ebbb55ea85adb12a\n",
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_projector.weight', 'vocab_layer_norm.weight', 'vocab_transform.weight', 'vocab_projector.bias', 'vocab_layer_norm.bias', 'vocab_transform.bias']\n",
      "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n",
      "/home/lolai/PycharmProjects/pythonProject/venv2/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 376\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 188\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "\u001B[32m[I 2022-07-09 00:33:48,731]\u001B[0m Trial 14 finished with value: 0.4407879849697681 and parameters: {'learning_rate': 0.0001412466602389466, 'weight_decay': 6.281311752926289e-05, 'num_train_epochs': 4}. Best is trial 13 with value: 0.4329967904598155.\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 30.8624, 'train_samples_per_second': 48.732, 'train_steps_per_second': 6.092, 'train_loss': 0.4407879849697681, 'epoch': 4.0}\n",
      "0.4329967904598155\n",
      "{'learning_rate': 0.00011715503310767902, 'weight_decay': 5.399698478570332e-05, 'num_train_epochs': 4}\n",
      "FrozenTrial(number=13, values=[0.4329967904598155], datetime_start=datetime.datetime(2022, 7, 9, 0, 32, 44, 396380), datetime_complete=datetime.datetime(2022, 7, 9, 0, 33, 16, 562733), params={'learning_rate': 0.00011715503310767902, 'weight_decay': 5.399698478570332e-05, 'num_train_epochs': 4}, distributions={'learning_rate': LogUniformDistribution(high=0.01, low=4e-05), 'weight_decay': LogUniformDistribution(high=0.01, low=4e-05), 'num_train_epochs': IntUniformDistribution(high=5, low=2, step=1)}, user_attrs={}, system_attrs={}, intermediate_values={}, trial_id=13, state=TrialState.COMPLETE, value=None)\n"
     ]
    }
   ],
   "source": [
    "study = optuna.create_study(study_name='hyper-parameter-search', direction='minimize')\n",
    "study.optimize(func=objective, n_trials=15)\n",
    "print(study.best_value)\n",
    "print(study.best_params)\n",
    "print(study.best_trial)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "data": {
      "text/plain": "{'learning_rate': 0.00011715503310767902,\n 'weight_decay': 5.399698478570332e-05,\n 'num_train_epochs': 4}"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_params"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Classify Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial GPU Usage\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 27% | 23% |\n",
      "GPU Usage after emptying the cache\n",
      "| ID | GPU | MEM |\n",
      "------------------\n",
      "|  0 | 40% | 25% |\n"
     ]
    }
   ],
   "source": [
    "from sentiment_classifier import classify_sent\n",
    "import pandas as pd\n",
    "import datasets"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "train_tsv = pd.read_csv('scicite/tsv/train.tsv', sep='\\t',\n",
    "                       names=[\"citingPaperID\", \"source\", \"string\", \"label\"]\n",
    "                       )\n",
    "test_tsv = pd.read_csv('scicite/tsv/test.tsv', sep='\\t',\n",
    "                       names=[\"citingPaperID\", \"source\", \"string\", \"label\"]\n",
    "                       )"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-ccba29a05905e5d2\n",
      "Reusing dataset csv (/home/lolai/.cache/huggingface/datasets/csv/default-ccba29a05905e5d2/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e)\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "629e8b90900946d5a83dab1c8aab1d65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-c286d4a61149d838\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/lolai/.cache/huggingface/datasets/csv/default-c286d4a61149d838/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2494f7bacd984e7985c7681b19f869c2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "db9c3cf07a83413688d21dae0b38f7e7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/lolai/.cache/huggingface/datasets/csv/default-c286d4a61149d838/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ac28fc407cc14ec2b9d32e9a361c022e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8cc3c7744943e4b7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /home/lolai/.cache/huggingface/datasets/csv/default-8cc3c7744943e4b7/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e...\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "78fbfb8d2fff404ab9db322f07b0d0a4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0082b84037034863b415cf59ce89aa6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/lolai/.cache/huggingface/datasets/csv/default-8cc3c7744943e4b7/0.0.0/6b9057d9e23d9d8a2f05b985917a0da84d70c5dae3d22ddd8a3f22fb01c69d9e. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/1 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d13eabc6f4184dc0b126561e44574749"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = datasets.load_dataset('csv', data_files={'train':'scicite/csv/train.csv'})\n",
    "dev_dataset = datasets.load_dataset('csv', data_files={'train':'scicite/csv/dev.csv'})\n",
    "test_dataset = datasets.load_dataset('csv', data_files={'train':'scicite/csv/test.csv'})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['text', 'label'],\n        num_rows: 8243\n    })\n})"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "train_sents = classify_sent(train_dataset['train']['text'], \"./Sentiment_Full_Model/model\")\n",
    "dev_sents = classify_sent(dev_dataset['train']['text'], \"./Sentiment_Full_Model/model\")\n",
    "test_sents = classify_sent(test_dataset['train']['text'], \"./Sentiment_Full_Model/model\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "train_dataset = train_dataset['train'].add_column(\"sentiment\", train_sents)\n",
    "dev_dataset = dev_dataset['train'].add_column(\"sentiment\", dev_sents)\n",
    "test_dataset = test_dataset['train'].add_column(\"sentiment\", test_sents)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "{'ID': 'fa7145adc9f8cfb8af7a189d9040c13c84ced094>20e23b4f76761d246a7c3b00b80e139e2008f77d_0',\n 'explicit': 'explicit',\n 'text': 'In addition the result of the present study supports previous studies which did not find increased rates of first-born children among individual with OCD (203134).',\n 'label': 'result',\n 'sentiment': 'positive'}"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset[1]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c39fb75031145a394f06613fc3e7f85"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c868c59984947beb8ce3647b0975666"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "Creating CSV from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "145ac3a11c6148aeb01e05adfccdb1a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "623996"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset.to_csv(\"train_sent.csv\")\n",
    "dev_dataset.to_csv(\"dev_sent.csv\")\n",
    "test_dataset.to_csv(\"test_sent.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}